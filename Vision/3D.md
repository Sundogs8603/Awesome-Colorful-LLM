# 3D

## Reading List

| Paper                                                                 | Base Language Model | Code                                                                         | Publication | Preprint                                    | Affiliation     |
| --------------------------------------------------------------------- | ------------------- | ---------------------------------------------------------------------------- | ----------- | ------------------------------------------- | --------------- |
| Point-Bind & Point-LLM: Aligning 3D with Multi-modality               | LLaMA               | [Point-Bind &amp; Point-LLM](https://github.com/ZiyuGuo99/Point-Bind_Point-LLM) |             | [2309.00615](https://arxiv.org/abs/2309.00615) | Shanghai AI Lab |
| PointLLM: Empowering Large Language Models to Understand Point Clouds | Vicuna              | [PointLLM](https://github.com/OpenRobotLab/PointLLM)                            |             | [2308.16911](https://arxiv.org/abs/2308.16911) | Shanghai AI Lab |
| RT-2: New model translates vision and language into action            | BLIP2               | [3D-LLM](https://github.com/UMass-Foundation-Model/3D-LLM)                      |             | [2307.12981](https://arxiv.org/abs/2307.12981) | UMASS           |
