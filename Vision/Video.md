*Table of Contents*

- [Video Understanding](#video-understanding)
  - [Reading List](#reading-list)
  - [Pretraining Tasks](#pretraining-tasks)
  - [Datasets](#datasets)
    - [Pretraining Corpora](#pretraining-corpora)
    - [Video Instructions](#video-instructions)
  - [Benchmarks](#benchmarks)
    - [Common Downstream Tasks](#common-downstream-tasks)
    - [Advanced Downstream Tasks](#advanced-downstream-tasks)
      - [Task-Specific Benchmarks](#task-specific-benchmarks)
      - [Multifaceted Benchmarks](#multifaceted-benchmarks)
- [Video Generation](#video-generation)
  - [Reading List](#reading-list-1)

# Video Understanding

## Reading List

**This reading list additionally collect video-language pretraining works before LLM**

*NOTEs: FT=Finetune, VidL=Video-Language, MM=Multimodal, INST=Instruction*

| Paper                                                                                                                     | Base Language Model       | Framework                        | Data                                                                  | Code                                                                                                     | Publication         | Preprint                                    | Affiliation     |
| ------------------------------------------------------------------------------------------------------------------------- | ------------------------- | -------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------- | ------------------------------------------- | --------------- |
| Video-LLaVA: Learning United Visual Representation by Alignment Before Projection                                         | Vicuna 1.5                | PT+FT                            | mixture                                                               | [Video-LLaVA](https://github.com/PKU-YuanGroup/Video-LLaVA)                                                 |                     | [2311.10122](https://arxiv.org/abs/2311.10122) | PKU             |
| Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding               | Vicuna                    | PT+FT                            | mixture                                                               | [Chat-UniVi](https://github.com/PKU-YuanGroup/Chat-UniVi)                                                   |                     | [2311.08046](https://arxiv.org/abs/2311.08046) | PKU             |
| LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment                     | -                         | PT                               | mixture                                                               | [LanguageBind](https://github.com/PKU-YuanGroup/LanguageBind)                                               |                     | [2310.01852](https://arxiv.org/abs/2310.01852) | PKU             |
| UniVTG: Towards Unified Video-Language Temporal Grounding                                                                 | CLIP                      | PT                               | mixture                                                               | [UniVTG](https://github.com/showlab/UniVTG)                                                                 | ICCV 2023           | [2307.16715](https://arxiv.org/abs/2307.16715) | NTU             |
| MovieChat: From Dense Token to Sparse Memory for Long Video Understanding                                                 | Vicuna                    | FT                               |                                                                       | [MovieChat](https://github.com/rese1f/MovieChat)                                                            |                     | [2307.16449](https://arxiv.org/abs/2307.16449) | Microsoft       |
| Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models                                | DeBerta                   | FT, RTr-Augmented                |                                                                       |                                                                                                          |                     | [2306.11732](https://arxiv.org/abs/2306.11732) | CUHK            |
| Macaw-LLM: Multi-Modal Language Modeling with Image, Video, Audio, and Text Integration                                   | LLaMA                     |                                  |                                                                       | [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM)                                                       |                     | [2306.09093](https://arxiv.org/abs/2306.09093) | Tencent         |
| Valley: Video assistant with large language model enhanced ability                                                        | Vicuna                    | PT, FT + MM-INST                 | [mixture](https://github.com/RupertLuo/Valley#train-valley-step-by-step) | [Valley](https://github.com/RupertLuo/Valley)                                                               |                     | [2306.07207](https://arxiv.org/abs/2306.07207) | ByteDance       |
| Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models                                  | Vicuna                    |                                  |                                                                       | [Video-ChatGPT](https://github.com/mbzuai-oryx/Video-ChatGPT)                                               |                     | [2306.05424](https://arxiv.org/abs/2306.05424) | MBZUAI          |
| Video-LLaMA: An Instruction-Finetuned Visual Language Model for Video Understanding                                       | LLaMA                     |                                  |                                                                       | [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA)                                                   |                     | [2306.02858](https://arxiv.org/abs/2306.02858) | Alibaba         |
| ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst                                          | Vicuna-13B                | FT + MM-INST                     |                                                                       | [ChatBridge](https://github.com/joez17/ChatBridge)                                                          |                     | [2305.16103](https://arxiv.org/abs/2305.16103) | CAS             |
| Self-Chained Image-Language Model for Video Localization and Question Answering                                           | BLIP2                     | 2-stage: localizer(LM) + answer | [QVHighlights](https://github.com/jayleicn/moment_detr), FT VidL         | [SeViLA](https://github.com/Yui010206/SeViLA)                                                               |                     | [2305.06988](https://arxiv.org/abs/2305.06988) | UNC             |
| VideoChat: Chat-Centric Video Understanding                                                                               | Blip2                     |                                  |                                                                       | [VideoChat](https://github.com/OpenGVLab/Ask-Anything)                                                      |                     | [2305.06355](https://arxiv.org/abs/2305.06355) | Shanghai AI Lab |
| X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages                     | ChatGPT                   |                                  |                                                                       | [X-LLM](https://github.com/phellonchen/X-LLM)                                                               |                     | [2305.04160](https://arxiv.org/abs/2305.04160) | CAS             |
| VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset                                                | Bert                      |                                  |                                                                       | [VALOR](https://github.com/TXH-mercury/VALOR)                                                               |                     | [2304.08345](https://arxiv.org/abs/2304.08345) | UCAS            |
| Verbs in Action: Improving verb understanding in video-language models                                                    | PaLM                      |                                  |                                                                       |                                                                                                          |                     | [2304.06708](https://arxiv.org/abs/2304.06708) | Google          |
| Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions                                                     | ChatGPT, Flan-T5 (BLIP2) |                                  |                                                                       | [ChatCaptioner](https://github.com/Vision-CAIR/ChatCaptioner)                                               |                     | [2304.04227](https://arxiv.org/abs/2304.04227) | KAUST           |
| Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering                                    | GPT2, GPT-Neo, GPT3       |                                  |                                                                       |                                                                                                          | CVPR2023 workshop   | [2304.03754](https://arxiv.org/abs/2304.03754) | Columbia Univ.  |
| Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning                                    | T5                        |                                  |                                                                       | [Vid2Seq](https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq)                      |                     | [2302.14115](https://arxiv.org/abs/2302.14115) | Google          |
| HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training                                                            | Bert                      |                                  |                                                                       |                                                                                                          |                     | [2212.14546](https://arxiv.org/abs/2212.14546) | Alibaba         |
| VindLU: A Recipe for Effective Video-and-Language Pretraining                                                             | Bert                      |                                  |                                                                       | [VindLU](https://github.com/klauscc/VindLU)                                                                 |                     | [2212.05051](https://arxiv.org/abs/2212.05051) | UNC             |
| SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training                                                | Bert                      |                                  |                                                                       |                                                                                                          |                     | [2211.11446](https://arxiv.org/abs/2211.11446) | UW              |
| CLOP: Video-and-Language Pre-Training with Knowledge Regularizations                                                      | Roberta                   |                                  |                                                                       |                                                                                                          | MM 2022             | [2211.03314](https://arxiv.org/abs/2211.03314) | Baidu           |
| Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning                                       | Bert                      |                                  |                                                                       |                                                                                                          | NIPS 2022           | [2210.06031](https://arxiv.org/abs/2210.06031) | Microsoft       |
| OmniVL: One Foundation Model for Image-Language and Video-Language Tasks                                                  | Bert                      |                                  |                                                                       |                                                                                                          | NIPS 2022           | [2209.07526](https://arxiv.org/abs/2209.07526) | Microsoft       |
| Clover: Towards A Unified Video-Language Alignment and Fusion Model                                                       | Bert                      |                                  |                                                                       | [Clover](https://github.com/LeeYN-43/Clover)                                                                |                     | [2207.07885](https://arxiv.org/abs/2207.07885) | Bytedance       |
| LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling                                               | Bert-like                 |                                  |                                                                       | [LAVENDER](https://github.com/microsoft/LAVENDER)                                                           | CVPR 2023           | [2206.07160](https://arxiv.org/abs/2206.07160) | Microsoft       |
| Revealing Single Frame Bias for Video-and-Language Learning                                                               | Bert                      |                                  |                                                                       | [Singularity](https://github.com/jayleicn/singularity)                                                      |                     | [2206.03428](https://arxiv.org/abs/2206.03428) | UNC             |
| Flamingo: a Visual Language Model for Few-Shot Learning                                                                   | Chinchilla                |                                  |                                                                       | [Flamingo](https://github.com/lucidrains/flamingo-pytorch)                                                  | NIPS 2022           | [2204.14198](https://arxiv.org/abs/2204.14198) | DeepMind        |
| All in One: Exploring Unified Video-Language Pre-training                                                                 | Bert-like                 |                                  |                                                                       | [All-In-One](https://github.com/showlab/all-in-one)                                                         | CVPR 2023           | [2203.07303](https://arxiv.org/abs/2203.07303) | NUS             |
| End-to-end Generative Pretraining for Multimodal Video Captioning                                                         | Bert+GPT2                 |                                  |                                                                       |                                                                                                          | CVPR 2022           | [2201.08264](https://arxiv.org/abs/2201.08264) | Google          |
| Align and Prompt: Video-and-Language Pre-training with Entity Prompts                                                     | Bert-like                 |                                  |                                                                       | [ALPRO](https://github.com/salesforce/ALPRO)                                                                | CVPR 2022           | [2112.09583](https://arxiv.org/abs/2112.09583) | Salesforce      |
| VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling,[V2](https://arxiv.org/pdf/2209.01540.pdf) | Bert                      |                                  |                                                                       | [VIOLET](https://github.com/tsujuifu/pytorch_violet)                                                        |                     | [2111.12681](https://arxiv.org/abs/2111.12681) | Microsoft       |
| VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding                                                | Bert                      |                                  |                                                                       | [VideoCLIP](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)                            | EMNLP 2021          | [2109.14084](https://arxiv.org/abs/2109.14084) | Facebook        |
| MERLOT: Multimodal Neural Script Knowledge Models,[V2](https://arxiv.org/abs/2201.02639)                                     | Roberta                   |                                  |                                                                       | [MERLOT](https://github.com/rowanz/merlot)                                                                  | NIPS 2021           | [2106.02636](https://arxiv.org/abs/2106.02636) | AI2             |
| VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding                                              | Bert                      |                                  |                                                                       | [VLP](https://github.com/facebookresearch/fairseq/blob/main/examples/MMPT/README.md)                        | ACL Findings 2021   | [2105.09996](https://arxiv.org/abs/2105.09996) | Facebook        |
| VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text                                 | Bert-like                 |                                  |                                                                       |                                                                                                          | NIPS 2021           | [2104.11178](https://arxiv.org/abs/2104.11178) | Google          |
| CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval                                                 | Bert-like                 |                                  |                                                                       | [CLIP4Clip](https://github.com/ArrowLuo/CLIP4Clip)                                                          | Neurocomputing 2022 | [2104.08860](https://arxiv.org/abs/2104.08860) | Microsoft       |
| Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval                                                  | Bert                      |                                  |                                                                       | [Frozen-in-Time](https://github.com/m-bain/frozen-in-time)                                                  | ICCV 2021           | [2104.00650](https://arxiv.org/abs/2104.00650) | Oxford          |
| Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling                                                | Bert                      |                                  |                                                                       | [ClipBert](https://github.com/jayleicn/ClipBERT)                                                            | CVPR 2021           | [2102.06183](https://arxiv.org/abs/2102.06183) | Microsoft       |
| ActBERT: Learning Global-Local Video-Text Representations                                                                 | Bert                      |                                  |                                                                       | [ActBert](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/en/model_zoo/multimodal/actbert.md) | CVPR 2020           | [2011.07231](https://arxiv.org/abs/2011.07231) | Baidu           |
| Video Understanding as Machine Translation                                                                                | T5                        |                                  |                                                                       |                                                                                                          |                     | [2006.07203](https://arxiv.org/abs/2006.07203) | Facebook        |
| HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training                                            | Bert                      |                                  |                                                                       | [HERO](https://github.com/linjieli222/HERO)                                                                 | EMNLP 2020          | [2005.00200](https://arxiv.org/abs/2005.00200) | Microsoft       |
| UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation                        | Bert                      |                                  |                                                                       | [UniVL](https://github.com/microsoft/UniVL)                                                                 |                     | [2002.06353](https://arxiv.org/abs/2002.06353) | Microsoft       |
| Learning Video Representations using Contrastive Bidirectional Transformer                                                | Bert                      |                                  |                                                                       |                                                                                                          |                     | [1906.05743](https://arxiv.org/abs/1906.05743) | Google          |
| VideoBERT: A Joint Model for Video and Language Representation Learning                                                   | Bert                      |                                  |                                                                       | [VideoBert (non-official)](https://github.com/ammesatyajit/VideoBERT)                                       | ICCV 2019           | [1904.01766](https://arxiv.org/abs/1904.01766) | Google          |

* [QVHighlights](https://github.com/jayleicn/moment_detr)
* [QVHighlights](https://github.com/jayleicn/moment_detr)

## Pretraining Tasks

*Commmonly Used Pretraining Tasks*

- Masked Language Modeling (MLM)
- Causal Language Modeling (LM)
- Masked Vision Modeling (MLM)
  - Vision = Frame
  - Vision = Patch
  - VIsion = Object
- Video Language Matching (VLM)
- Video Language Contrastive (VLC)

## Datasets and Benchmarks

### Pretraining Corpora

| Paper                                                                                                | Video Clips | Duration | Sentences | Domain      | Download Link                                                              |
| ---------------------------------------------------------------------------------------------------- | ----------- | -------- | --------- | ----------- | -------------------------------------------------------------------------- |
| Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval                             | 2.5M        | 18s      | 2.5M      | open        | [WebVid-2M](https://github.com/m-bain/webvid)                                 |
| Howto100m: Learning a text-video embedding by watching hundred million narrated video clips          | 136M        | 4s       | 136M      | instruction | [HowTo100M](https://www.di.ens.fr/willow/research/howto100m/)                 |
| Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing         | 6M          | -20m     | ~720M     | open        | [YT-Temporal-180M](https://rowanzellers.com/merlot/)                          |
| Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions        | 100M        | 13.4s    | 100M      | open        | [HD-VILA-100M](https://github.com/microsoft/XPretrain/tree/main/hd-vila-100m) |
| CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos                              | 18M         | 60s      |           | open        | [YTD-18M](https://seungjuhan.me/champagne/)                                   |
| Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks | 10 M        | 54.2s    | 10 M      | open        | [Youku-mPLUG](https://github.com/X-PLUG/Youku-mPLUG#download)                 |

### Video Instructions

| Dataset                                                                                              | Domain     | Source                         |
| ---------------------------------------------------------------------------------------------------- | ---------- | ------------------------------ |
| [Instruction from Video-ChatGPT](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/data/README.md) | general QA | ActivityNet + Human Annotation |
|                                                                                                      |            |                                |

## Benchmarks

### Common Downstream Tasks

| **Task** | Paper                                                                                                         | Download Link                                                                                                               | Publication |
| -------------- | ------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ----------- |
| Retrieval      | Collecting Highly Parallel Data for Paraphrase Evaluation                                                     | [MSVD](https://www.cs.utexas.edu/users/ml/clamp/videoDescription/)                                                             | ACL 2011    |
| Retrieval      | A Dataset for Movie Description                                                                               | [LSMDC](https://sites.google.com/site/describingmovies/download)                                                               | CVPR 2015   |
| Retrieval      | MSR-VTT: A Large Video Description Dataset for Bridging Video and Language                                    | [MSR-VTT](https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip)                                                 | CVPR 2016   |
| Retrieval      | Localizing Moments in Video with Natural Language                                                             | [DiDeMo](https://github.com/LisaAnne/LocalizingMoments)                                                                        | ICCV 2017   |
| Retrieval      | Dense-Captioning Events in Videos                                                                             | [ActivityNet Caption](https://cs.stanford.edu/people/ranjaykrishna/densevid/)                                                  | ICCV 2017   |
| Retrieval      | Towards Automatic Learning of Procedures from Web Instructional Videos                                        | [YouCook2](http://youcook2.eecs.umich.edu/download)                                                                            | AAAI 2018   |
| OE QA          | TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering                                        | [TGIF-Frame](https://github.com/YunseokJANG/tgif-qa/tree/cvpr2017/dataset)                                                     | CVPR 2017   |
| OE QA          | A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering | [LSMDC-FiB](https://github.com/yj-yu/lsmdc)                                                                                    | CVPR 2017   |
| OE QA          | Video Question Answering via Gradually Refined Attention over Appearance and Motion                           | [MSRVTT-QA](https://github.com/xudejing/video-question-answering),[MSVD-QA](https://github.com/xudejing/video-question-answering) | MM 2017     |
| OE QA          | ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering                         | [ActivityNet-QA](https://github.com/MILVLG/activitynet-qa)                                                                     | AAAI 2019   |
| MC QA          | Learning Language-Visual Embedding for Movie Understanding with Natural-Language                              | [LSMDC-MC](https://github.com/yj-yu/lsmdc)                                                                                     |             |
| MC  QA         | TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering                                        | [TGIF-Action, TGIF-Transition](https://github.com/YunseokJANG/tgif-qa/tree/cvpr2017/dataset)                                   | CVPR 2017   |
| MC QA          | A Joint Sequence Fusion Model for Video Question Answering and Retrieval                                      | [MSRVTT-MC](https://github.com/yj-yu/lsmdc)                                                                                    | ECCV 2018   |
| Caption        | Collecting Highly Parallel Data for Paraphrase Evaluation                                                     | [MSVD](https://www.cs.utexas.edu/users/ml/clamp/videoDescription/)                                                             | ACL 2011    |
| Caption        | MSR-VTT: A Large Video Description Dataset for Bridging Video and Language                                    | [MSR-VTT](https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip)                                                 | CVPR 2016   |
| Dense Caption  | Dense-Captioning Events in Videos                                                                             | [ActivityNet Caption](https://cs.stanford.edu/people/ranjaykrishna/densevid/)                                                  | ICCV 2017   |
| Dense Caption  | Towards Automatic Learning of Procedures from Web Instructional Videos                                        | [YouCook2](http://youcook2.eecs.umich.edu/download)                                                                            | AAAI 2018   |
| Dense Caption  | Multimodal Pretraining for Dense Video Captioning                                                             | [ViTT](https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT)                                                   | AACL 2020   |

### Advanced Downstream Tasks

#### Task-Specific Benchmarks

| paper                                                                                                          | task                     | duration | domain       | link                                                                                         | publication |
| -------------------------------------------------------------------------------------------------------------- | ------------------------ | -------- | ------------ | -------------------------------------------------------------------------------------------- | ----------- |
| From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering | Video QA                 | 9s       | open         | [Causal-VidQA](https://github.com/bcmi/Causal-VidQA)                                            | CVPR 2022   |
| VIOLIN: A Large-Scale Dataset for Video-and-Language Inference                                                 | Video Language Inference | 35.2s    | movie        | [VIOLIN](https://github.com/jimmy646/violin)                                                    | CVPR 2020   |
| TVQA: Localized, Compositional Video Question Answering                                                        | Video QA                 | 60-90s   | movie        | [TVQA](https://tvqa.cs.unc.edu/)                                                                | EMNLP 2018  |
| AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning                                                  | Video QA                 | 30s      | open         | [AGQA](https://cs.stanford.edu/people/ranjaykrishna/agqa/)                                      | CVPR 2021   |
| NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions                                       | Video QA                 | 44s      | open         | [NExT-QA-MC](https://github.com/doc-doc/NExT-QA), [NExT-QA-OE](https://github.com/doc-doc/NExT-OE) | CVPR 2021   |
| STAR: A Benchmark for Situated Reasoning in Real-World Videos                                                  | Video QA                 | 12s      | open         | [Star](https://github.com/csbobby/STAR_Benchmark)                                               | NIPS 2021   |
| Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments           | Video QA                 | 20s      | virtual env. | [Env-QA](https://envqa.github.io/)                                                              | ICCV 2021   |
| Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence                                   | Video QA                 | 60s      | open         | [Social-IQ](https://www.thesocialiq.com/)                                                       | CVPR 2019   |

#### Multifaceted Benchmarks

| Benchmark                                                | Task                | Data    | Paper | Preprint | Publication | Affiliation |
| -------------------------------------------------------- | ------------------- | ------- | ----- | -------- | ----------- | ----------- |
| [Video-Bench](https://github.com/PKU-YuanGroup/Video-Bench) | MC (general domain) | mixture |       |          |             |             |

## Projects & Tools

- [Awesome-Video-Object-Segmentation](https://github.com/gaomingqi/Awesome-Video-Object-Segmentation), A curated list of video object segmentation (vos) papers, datasets, and projects.

# Video Generation

## Reading List

Survey

- (2023-10) A Survey on Video Diffusion Models  [paper](https://arxiv.org/abs/2310.10647)  [repo](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models)

Reading List

| Paper                                                                             | Base Structure      | Data           | Code                                       | Publication | Preprint                                    | Affiliation |
| --------------------------------------------------------------------------------- | ------------------- | --------- | -------------- | ------------------------------------------ | ----------- | ------------------------------------------- | ----------- |
| VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning | GPT4 + UNet         |                     |           [VideoDirectorGPT](https://github.com/HL-hanlin/VideoDirectorGPT)                                 |             |                     [2309.15091](https://arxiv.org/abs/2309.15091)                        |      UNC       |
| CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers   | Transformer + VQVAE      | self-construct | [CogVideo](https://github.com/THUDM/CogVideo) |             | [2205.15868](https://arxiv.org/abs/2205.15868) | THU         |
