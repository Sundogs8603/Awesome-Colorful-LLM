# Video

*Table of Contents*

- [Video Understanding](#video-understanding)
  - [Reading List](#reading-list)
  - [Pretraining Tasks](#pretraining-tasks)
  - [Datasets](#datasets)
    - [Pretraining Corpora](#pretraining-corpora)
    - [Video Instructions](#video-instructions)
  - [Benchmarks](#benchmarks)
    - [Common Downstream Tasks](#common-downstream-tasks)
    - [Advanced Downstream Tasks](#advanced-downstream-tasks)
      - [Task-Specific Benchmarks](#task-specific-benchmarks)
      - [Multifaceted Benchmarks](#multifaceted-benchmarks)
  - [Metrics](#metrics)
  - [Projects & Tools](#projects--tools)
- [Video Generation](#video-generation)
  - [Reading List](#reading-list-1)
  - [Metrics](#metrics-1)
  - [Projects](#projects)

## Reading List

**This reading list additionally collect video-language pretraining works before LLM**

*NOTEs: FT=Finetune, VidL=Video-Language, MM=Multimodal, INST=Instruction*

| Paper                                                                                                                     | Base Language Model       | Framework                        | Data                                                                  | Code                                                                                                     | Publication         | Preprint                                    | Affiliation     |
| ------------------------------------------------------------------------------------------------------------------------- | ------------------------- | -------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------- | ------------------------------------------- | --------------- |
| VideoMamba: State Space Model for Efficient Video Understanding                                                           | -                         | -                                |                                                                       | [VideoMamba](https://github.com/OpenGVLab/VideoMamba)                                                       |                     | [2403.06977](https://arxiv.org/abs/2403.06977) | Shanghai AI Lab |
| VideoPrism: A Foundational Visual Encoder for Video Understanding                                                         | (PaLM)                    | PT                               | mixture                                                               |                                                                                                          |                     | [2402.13217](https://arxiv.org/abs/2402.13217) | Google          |
| Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning                                       | llama                     | self-construct(Moment-10M)       | MMINST+temporal decode                                                | [Momentor](https://github.com/DCDmllm/Momentor)                                                             |                     | [2402.11435](https://arxiv.org/abs/2402.11435) | ZJU             |
| World Model on Million-Length Video And Language With RingAttention                                                       | LLaMA2                    | PT+FT                            | mixture                                                               | [LWM](https://github.com/LargeWorldModel/LWM)                                                               |                     | [2402.08268](https://arxiv.org/abs/2402.08268) | UCB             |
| Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization                              | LLaMA2                    | PT+FT                            | mixture                                                               | [LaVIT](https://github.com/jy0205/LaVIT)                                                                    |                     | [2402.03161](https://arxiv.org/abs/2402.03161) | PKU             |
| MoE-LLaVA: Mixture of Experts for Large Vision-Language Models                                                            | StableLM, Qwen, Phi2      | MoE                              | mixture (MM-INST)                                                     | [MoE-LLaVA](https://github.com/PKU-YuanGroup/MoE-LLaVA)                                                     |                     | [2401.15947](https://arxiv.org/abs/2401.15947) | PKU             |
| TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding                                   | LLaMA2                    | MM-INST                          | mixture (additional Transcribed Speeck)                               | [TimeChat](https://github.com/RenShuhuai-Andy/TimeChat)                                                     |                     | [2312.02051](https://arxiv.org/abs/2312.02051) | PKU             |
| VTimeLLM: Empower LLM to Grasp Video Moments                                                                              | Vicuna                    | INST+temporal                    | mixture                                                               | [VTimeLLM](https://github.com/huangb23/VTimeLLM)                                                            |                     | [2311.18445](https://arxiv.org/abs/2311.18445) | THU             |
| LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models                                                            | Vicuna                    | MM-INST                          | self-construct                                                        | [LLaMA-VID](https://github.com/dvlab-research/LLaMA-VID)                                                    |                     | [2311.17043](https://arxiv.org/abs/2311.17043) | CUHK            |
| Video-LLaVA: Learning United Visual Representation by Alignment Before Projection                                         | Vicuna 1.5                | PT+FT                            | mixture                                                               | [Video-LLaVA](https://github.com/PKU-YuanGroup/Video-LLaVA)                                                 |                     | [2311.10122](https://arxiv.org/abs/2311.10122) | PKU             |
| Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding               | Vicuna                    | PT+FT                            | mixture                                                               | [Chat-UniVi](https://github.com/PKU-YuanGroup/Chat-UniVi)                                                   |                     | [2311.08046](https://arxiv.org/abs/2311.08046) | PKU             |
| LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment                     | -                         | PT                               | mixture                                                               | [LanguageBind](https://github.com/PKU-YuanGroup/LanguageBind)                                               |                     | [2310.01852](https://arxiv.org/abs/2310.01852) | PKU             |
| UniVTG: Towards Unified Video-Language Temporal Grounding                                                                 | CLIP                      | PT                               | mixture                                                               | [UniVTG](https://github.com/showlab/UniVTG)                                                                 | ICCV 2023           | [2307.16715](https://arxiv.org/abs/2307.16715) | NTU             |
| MovieChat: From Dense Token to Sparse Memory for Long Video Understanding                                                 | Vicuna                    | FT                               |                                                                       | [MovieChat](https://github.com/rese1f/MovieChat)                                                            |                     | [2307.16449](https://arxiv.org/abs/2307.16449) | Microsoft       |
| Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models                                | DeBerta                   | FT, RTr-Augmented                |                                                                       |                                                                                                          |                     | [2306.11732](https://arxiv.org/abs/2306.11732) | CUHK            |
| Macaw-LLM: Multi-Modal Language Modeling with Image, Video, Audio, and Text Integration                                   | LLaMA                     |                                  |                                                                       | [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM)                                                       |                     | [2306.09093](https://arxiv.org/abs/2306.09093) | Tencent         |
| Valley: Video assistant with large language model enhanced ability                                                        | Vicuna                    | PT, FT + MM-INST                 | [mixture](https://github.com/RupertLuo/Valley#train-valley-step-by-step) | [Valley](https://github.com/RupertLuo/Valley)                                                               |                     | [2306.07207](https://arxiv.org/abs/2306.07207) | ByteDance       |
| Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models                                  | Vicuna                    |                                  |                                                                       | [Video-ChatGPT](https://github.com/mbzuai-oryx/Video-ChatGPT)                                               |                     | [2306.05424](https://arxiv.org/abs/2306.05424) | MBZUAI          |
| Video-LLaMA: An Instruction-Finetuned Visual Language Model for Video Understanding                                       | LLaMA                     |                                  |                                                                       | [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA)                                                   |                     | [2306.02858](https://arxiv.org/abs/2306.02858) | Alibaba         |
| ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst                                          | Vicuna-13B                | FT + MM-INST                     |                                                                       | [ChatBridge](https://github.com/joez17/ChatBridge)                                                          |                     | [2305.16103](https://arxiv.org/abs/2305.16103) | CAS             |
| Self-Chained Image-Language Model for Video Localization and Question Answering                                           | BLIP2                     | 2-stage: localizer(LM) + answer | [QVHighlights](https://github.com/jayleicn/moment_detr), FT VidL         | [SeViLA](https://github.com/Yui010206/SeViLA)                                                               |                     | [2305.06988](https://arxiv.org/abs/2305.06988) | UNC             |
| VideoChat: Chat-Centric Video Understanding                                                                               | Blip2                     |                                  |                                                                       | [VideoChat](https://github.com/OpenGVLab/Ask-Anything)                                                      |                     | [2305.06355](https://arxiv.org/abs/2305.06355) | Shanghai AI Lab |
| X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages                     | ChatGPT                   |                                  |                                                                       | [X-LLM](https://github.com/phellonchen/X-LLM)                                                               |                     | [2305.04160](https://arxiv.org/abs/2305.04160) | CAS             |
| VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset                                                | Bert                      |                                  |                                                                       | [VALOR](https://github.com/TXH-mercury/VALOR)                                                               |                     | [2304.08345](https://arxiv.org/abs/2304.08345) | UCAS            |
| Verbs in Action: Improving verb understanding in video-language models                                                    | PaLM                      |                                  |                                                                       |                                                                                                          |                     | [2304.06708](https://arxiv.org/abs/2304.06708) | Google          |
| Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions                                                     | ChatGPT, Flan-T5 (BLIP2) |                                  |                                                                       | [ChatCaptioner](https://github.com/Vision-CAIR/ChatCaptioner)                                               |                     | [2304.04227](https://arxiv.org/abs/2304.04227) | KAUST           |
| Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering                                    | GPT2, GPT-Neo, GPT3       |                                  |                                                                       |                                                                                                          | CVPR2023 workshop   | [2304.03754](https://arxiv.org/abs/2304.03754) | Columbia Univ.  |
| Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning                                    | T5                        |                                  |                                                                       | [Vid2Seq](https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq)                      |                     | [2302.14115](https://arxiv.org/abs/2302.14115) | Google          |
| HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training                                                            | Bert                      |                                  |                                                                       |                                                                                                          |                     | [2212.14546](https://arxiv.org/abs/2212.14546) | Alibaba         |
| VindLU: A Recipe for Effective Video-and-Language Pretraining                                                             | Bert                      |                                  |                                                                       | [VindLU](https://github.com/klauscc/VindLU)                                                                 |                     | [2212.05051](https://arxiv.org/abs/2212.05051) | UNC             |
| Learning Video Representations from Large Language Models                                                                 | GPT2                      | PT (data-augment)                | Ego4D/HowTo100M                                                       | [LaViLa](https://github.com/facebookresearch/LaViLa)                                                        | CVPR2023            | [2212.04501](https://arxiv.org/abs/2212.04501) | Meta            |
| SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training                                                | Bert                      |                                  |                                                                       |                                                                                                          |                     | [2211.11446](https://arxiv.org/abs/2211.11446) | UW              |
| CLOP: Video-and-Language Pre-Training with Knowledge Regularizations                                                      | Roberta                   |                                  |                                                                       |                                                                                                          | MM 2022             | [2211.03314](https://arxiv.org/abs/2211.03314) | Baidu           |
| Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning                                       | Bert                      |                                  |                                                                       |                                                                                                          | NIPS 2022           | [2210.06031](https://arxiv.org/abs/2210.06031) | Microsoft       |
| OmniVL: One Foundation Model for Image-Language and Video-Language Tasks                                                  | Bert                      |                                  |                                                                       |                                                                                                          | NIPS 2022           | [2209.07526](https://arxiv.org/abs/2209.07526) | Microsoft       |
| Clover: Towards A Unified Video-Language Alignment and Fusion Model                                                       | Bert                      |                                  |                                                                       | [Clover](https://github.com/LeeYN-43/Clover)                                                                |                     | [2207.07885](https://arxiv.org/abs/2207.07885) | Bytedance       |
| LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling                                               | Bert-like                 |                                  |                                                                       | [LAVENDER](https://github.com/microsoft/LAVENDER)                                                           | CVPR 2023           | [2206.07160](https://arxiv.org/abs/2206.07160) | Microsoft       |
| Revealing Single Frame Bias for Video-and-Language Learning                                                               | Bert                      |                                  |                                                                       | [Singularity](https://github.com/jayleicn/singularity)                                                      |                     | [2206.03428](https://arxiv.org/abs/2206.03428) | UNC             |
| Label-Efficient Online Continual Object Detection in Streaming Video                                                      | -                         |                                  | (continual)                                                           | [Efficient-CLS](https://github.com/showlab/Efficient-CLS)                                                   | ICCV 2023           | [2206.00309](https://arxiv.org/abs/2206.00309) | NUS             |
| Flamingo: a Visual Language Model for Few-Shot Learning                                                                   | Chinchilla                |                                  |                                                                       | [Flamingo](https://github.com/lucidrains/flamingo-pytorch)                                                  | NIPS 2022           | [2204.14198](https://arxiv.org/abs/2204.14198) | DeepMind        |
| All in One: Exploring Unified Video-Language Pre-training                                                                 | Bert-like                 |                                  |                                                                       | [All-In-One](https://github.com/showlab/all-in-one)                                                         | CVPR 2023           | [2203.07303](https://arxiv.org/abs/2203.07303) | NUS             |
| End-to-end Generative Pretraining for Multimodal Video Captioning                                                         | Bert+GPT2                 |                                  |                                                                       |                                                                                                          | CVPR 2022           | [2201.08264](https://arxiv.org/abs/2201.08264) | Google          |
| Align and Prompt: Video-and-Language Pre-training with Entity Prompts                                                     | Bert-like                 |                                  |                                                                       | [ALPRO](https://github.com/salesforce/ALPRO)                                                                | CVPR 2022           | [2112.09583](https://arxiv.org/abs/2112.09583) | Salesforce      |
| VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling,[V2](https://arxiv.org/pdf/2209.01540.pdf) | Bert                      |                                  |                                                                       | [VIOLET](https://github.com/tsujuifu/pytorch_violet)                                                        |                     | [2111.12681](https://arxiv.org/abs/2111.12681) | Microsoft       |
| VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding                                                | Bert                      |                                  |                                                                       | [VideoCLIP](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)                            | EMNLP 2021          | [2109.14084](https://arxiv.org/abs/2109.14084) | Facebook        |
| MERLOT: Multimodal Neural Script Knowledge Models,[V2](https://arxiv.org/abs/2201.02639)                                     | Roberta                   |                                  |                                                                       | [MERLOT](https://github.com/rowanz/merlot)                                                                  | NIPS 2021           | [2106.02636](https://arxiv.org/abs/2106.02636) | AI2             |
| VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding                                              | Bert                      |                                  |                                                                       | [VLP](https://github.com/facebookresearch/fairseq/blob/main/examples/MMPT/README.md)                        | ACL Findings 2021   | [2105.09996](https://arxiv.org/abs/2105.09996) | Facebook        |
| VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text                                 | Bert-like                 |                                  |                                                                       |                                                                                                          | NIPS 2021           | [2104.11178](https://arxiv.org/abs/2104.11178) | Google          |
| CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval                                                 | Bert-like                 |                                  |                                                                       | [CLIP4Clip](https://github.com/ArrowLuo/CLIP4Clip)                                                          | Neurocomputing 2022 | [2104.08860](https://arxiv.org/abs/2104.08860) | Microsoft       |
| Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval                                                  | Bert                      |                                  |                                                                       | [Frozen-in-Time](https://github.com/m-bain/frozen-in-time)                                                  | ICCV 2021           | [2104.00650](https://arxiv.org/abs/2104.00650) | Oxford          |
| Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling                                                | Bert                      |                                  |                                                                       | [ClipBert](https://github.com/jayleicn/ClipBERT)                                                            | CVPR 2021           | [2102.06183](https://arxiv.org/abs/2102.06183) | Microsoft       |
| ActBERT: Learning Global-Local Video-Text Representations                                                                 | Bert                      |                                  |                                                                       | [ActBert](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/en/model_zoo/multimodal/actbert.md) | CVPR 2020           | [2011.07231](https://arxiv.org/abs/2011.07231) | Baidu           |
| Video Understanding as Machine Translation                                                                                | T5                        |                                  |                                                                       |                                                                                                          |                     | [2006.07203](https://arxiv.org/abs/2006.07203) | Facebook        |
| HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training                                            | Bert                      |                                  |                                                                       | [HERO](https://github.com/linjieli222/HERO)                                                                 | EMNLP 2020          | [2005.00200](https://arxiv.org/abs/2005.00200) | Microsoft       |
| UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation                        | Bert                      |                                  |                                                                       | [UniVL](https://github.com/microsoft/UniVL)                                                                 |                     | [2002.06353](https://arxiv.org/abs/2002.06353) | Microsoft       |
| Learning Video Representations using Contrastive Bidirectional Transformer                                                | Bert                      |                                  |                                                                       |                                                                                                          |                     | [1906.05743](https://arxiv.org/abs/1906.05743) | Google          |
| VideoBERT: A Joint Model for Video and Language Representation Learning                                                   | Bert                      |                                  |                                                                       | [VideoBert (non-official)](https://github.com/ammesatyajit/VideoBERT)                                       | ICCV 2019           | [1904.01766](https://arxiv.org/abs/1904.01766) | Google          |

## Pretraining Tasks

*Commmonly Used Pretraining Tasks*

- Masked Language Modeling (MLM)
- Causal Language Modeling (LM)
- Masked Vision Modeling (MLM)
  - Vision = Frame
  - Vision = Patch
  - VIsion = Object
- Video Language Matching (VLM)
- Video Language Contrastive (VLC)

## Datasets

### Pretraining Corpora

| Paper                                                                                                     | Video Clips  | Duration | Sentences | Domain                | Download Link                                                               |
| --------------------------------------------------------------------------------------------------------- | ------------ | -------- | --------- | --------------------- | --------------------------------------------------------------------------- |
| (❗NOT AVAILABLE, 23 Feb 2024) Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval | 2.5M (2M)    | 18s      | 2.5M      | open (web)            | [WebVid-2M](https://github.com/m-bain/webvid), WebVid-10M                      |
| Howto100m: Learning a text-video embedding by watching hundred million narrated video clips               | 136M (1.2M)  | 4s       | 136M      | instruction (YouTube) | [HowTo100M](https://www.di.ens.fr/willow/research/howto100m/)                  |
| Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing              | 180M (6M)    | -20m     | ~720M     | open (YouTube)        | [YT-Temporal-180M](https://rowanzellers.com/merlot/)                           |
| Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions             | 100M (3.3M)  | 13.4s    | 100M      | open (YouTube)        | [HD-VILA-100M](https://github.com/microsoft/XPretrain/tree/main/hd-vila-100m)  |
| Learning audio-video modalities from image captions                                                       | 10.3M (6.3M) | 10s      |           | open (web)            | [VideoCC](https://github.com/google-research-datasets/videoCC-data)            |
| CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos                                   | 18M          | 60s      |           | open (YouTube)        | [YTD-18M](https://seungjuhan.me/champagne/)                                    |
| Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks      | 10 M         | 54.2s    | 10 M      | open (YOUKU)          | [Youku-mPLUG](https://github.com/X-PLUG/Youku-mPLUG#download)                  |
| InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation                   | 234M (7.1M)  | 11.7s    | 234 M     | open (YouTube)        | [InternVid](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) |
| Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers                                    | 70M          | 8.5s     | 70M       | open                  | [Panda-70M](https://github.com/snap-research/Panda-70M)  `from HD-VILA-100M` |

### Video Instructions

| Dataset                                                                             | Statistics           | Source                                        |
| ----------------------------------------------------------------------------------- | -------------------- | --------------------------------------------- |
| [Video-ChatGPT](https://github.com/mbzuai-oryx/Video-ChatGPT/blob/main/data/README.md) | 100k INST/10k videos | ActivityNet + (Human+GPT) Annotation          |
| [Valley](https://github.com/RupertLuo/Valley)                                          | 65k INST/100k videos | (VATEX + JukinMedia )+ (Human+GPT) Annotation |
| [VideoChat](https://github.com/OpenGVLab/Ask-Anything)                                 | 11k INST/11k videos  | WebVid + GPT Annotation                       |
| [TimeIT](https://github.com/RenShuhuai-Andy/TimeChat?tab=readme-ov-file#data)          | 125k INST            | Mixture + GPT Annotation                     |

### Others

- [Neurips23 D&B] [VidChapters](https://github.com/antoyang/VidChapters), a large-scale dataset of user-chaptered videos. We study three tasks on top of this dataset and show that video chapter generation models trained on VidChapters-7M transfer well to dense video captioning.

## Benchmarks

### Common Downstream Tasks

| **Task** | Paper                                                                                                         | Download Link                                                                                                               | Publication |
| -------------- | ------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ----------- |
| Retrieval      | Collecting Highly Parallel Data for Paraphrase Evaluation                                                     | [MSVD](https://www.cs.utexas.edu/users/ml/clamp/videoDescription/)                                                             | ACL 2011    |
| Retrieval      | A Dataset for Movie Description                                                                               | [LSMDC](https://sites.google.com/site/describingmovies/download)                                                               | CVPR 2015   |
| Retrieval      | MSR-VTT: A Large Video Description Dataset for Bridging Video and Language                                    | [MSR-VTT](https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip)                                                 | CVPR 2016   |
| Retrieval      | Localizing Moments in Video with Natural Language                                                             | [DiDeMo](https://github.com/LisaAnne/LocalizingMoments)                                                                        | ICCV 2017   |
| Retrieval      | Dense-Captioning Events in Videos                                                                             | [ActivityNet Caption](https://cs.stanford.edu/people/ranjaykrishna/densevid/)                                                  | ICCV 2017   |
| Retrieval      | Towards Automatic Learning of Procedures from Web Instructional Videos                                        | [YouCook2](http://youcook2.eecs.umich.edu/download)                                                                            | AAAI 2018   |
| OE QA          | TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering                                        | [TGIF-Frame](https://github.com/YunseokJANG/tgif-qa/tree/cvpr2017/dataset)                                                     | CVPR 2017   |
| OE QA          | A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering | [LSMDC-FiB](https://github.com/yj-yu/lsmdc)                                                                                    | CVPR 2017   |
| OE QA          | Video Question Answering via Gradually Refined Attention over Appearance and Motion                           | [MSRVTT-QA](https://github.com/xudejing/video-question-answering),[MSVD-QA](https://github.com/xudejing/video-question-answering) | MM 2017     |
| OE QA          | ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering                         | [ActivityNet-QA](https://github.com/MILVLG/activitynet-qa)                                                                     | AAAI 2019   |
| MC QA          | Learning Language-Visual Embedding for Movie Understanding with Natural-Language                              | [LSMDC-MC](https://github.com/yj-yu/lsmdc)                                                                                     |             |
| MC  QA         | TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering                                        | [TGIF-Action, TGIF-Transition](https://github.com/YunseokJANG/tgif-qa/tree/cvpr2017/dataset)                                   | CVPR 2017   |
| MC QA          | A Joint Sequence Fusion Model for Video Question Answering and Retrieval                                      | [MSRVTT-MC](https://github.com/yj-yu/lsmdc)                                                                                    | ECCV 2018   |
| Caption        | Collecting Highly Parallel Data for Paraphrase Evaluation                                                     | [MSVD](https://www.cs.utexas.edu/users/ml/clamp/videoDescription/)                                                             | ACL 2011    |
| Caption        | MSR-VTT: A Large Video Description Dataset for Bridging Video and Language                                    | [MSR-VTT](https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip)                                                 | CVPR 2016   |
| Caption        | VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research                       | [VATEX](https://eric-xw.github.io/vatex-website/explore.html)                                                                  | ICCV 2019   |
| Dense Caption  | Dense-Captioning Events in Videos                                                                             | [ActivityNet Caption](https://cs.stanford.edu/people/ranjaykrishna/densevid/)                                                  | ICCV 2017   |
| Dense Caption  | Towards Automatic Learning of Procedures from Web Instructional Videos                                        | [YouCook2](http://youcook2.eecs.umich.edu/download)                                                                            | AAAI 2018   |
| Dense Caption  | Multimodal Pretraining for Dense Video Captioning                                                             | [ViTT](https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT)                                                   | AACL 2020   |
| Action         | HMDB: A large video database for human motion recognition                                                     | [HMDB](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)-51                                       | ICCV 2021   |
| Action         | UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild                                        | [UCF](https://www.crcv.ucf.edu/data/UCF101.php)-101                                                                            | ICCV 2013   |
| Action         | ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding                                   | [ActivityNet](http://activity-net.org/about.html)-200                                                                          | CVPR 2015   |
| Action         | Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding                                  | [Charades](https://prior.allenai.org/projects/charades)-157                                                                    | ECCV 2016   |
| Action         | The Kinetics Human Action Video Dataset                                                                       | [Kinetics](https://github.com/cvdfoundation/kinetics-dataset)-400/600/700                                                      |             |

### Advanced Downstream Tasks

#### Task-Specific Benchmarks

| paper                                                                                                          | task                     | duration    | domain          | link                                                                                         | publication |
| -------------------------------------------------------------------------------------------------------------- | ------------------------ | ----------- | --------------- | -------------------------------------------------------------------------------------------- | ----------- |
| MovieChat: From Dense Token to Sparse Memory for Long Video Understanding                                      | Video QA                 | ~8m         | movie           | [MovieChat](https://github.com/rese1f/MovieChat)                                                | ZJU         |
| ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models                    | Temporal Grounding       | ~60s (mix.) | open            | [ViLMA](https://github.com/ilkerkesen/ViLMA)                                                    | ICLR 2024   |
| From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering | Video QA                 | 9s          | open            | [Causal-VidQA](https://github.com/bcmi/Causal-VidQA)                                            | CVPR 2022   |
| VIOLIN: A Large-Scale Dataset for Video-and-Language Inference                                                 | Video Language Inference | 35.2s       | movie           | [VIOLIN](https://github.com/jimmy646/violin)                                                    | CVPR 2020   |
| TVQA: Localized, Compositional Video Question Answering                                                        | Video QA                 | 60-90s      | movie           | [TVQA](https://tvqa.cs.unc.edu/)                                                                | EMNLP 2018  |
| AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning                                                  | Video QA                 | 30s         | open            | [AGQA](https://cs.stanford.edu/people/ranjaykrishna/agqa/)                                      | CVPR 2021   |
| NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions                                       | Video QA                 | 44s         | open            | [NExT-QA-MC](https://github.com/doc-doc/NExT-QA), [NExT-QA-OE](https://github.com/doc-doc/NExT-OE) | CVPR 2021   |
| Towards Long-Form Video Understanding                                                                          | Classification           | 1-3m        | movie           | [LVU](https://github.com/chaoyuaw/lvu)                                                          | CVPR 2021   |
| STAR: A Benchmark for Situated Reasoning in Real-World Videos                                                  | Video QA                 | 12s         | open            | [Star](https://github.com/csbobby/STAR_Benchmark)                                               | NIPS 2021   |
| Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments           | Video QA                 | 20s         | virtual env.    | [Env-QA](https://envqa.github.io/)                                                              | ICCV 2021   |
| COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis                                     | Localization/Action Seg. | 3.36m       | open (instruct) | [COIN](https://coin-dataset.github.io/)                                                         | CVPR 2019   |
| Cross-task weakly supervised learning from instructional videos                                                | Localization             | 4m57s       | open (instruct) | [CrossTask](https://github.com/DmZhukov/CrossTask)                                              | CVPR 2019   |
| Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence                                   | Video QA                 | 60s         | open            | [Social-IQ](https://www.thesocialiq.com/)                                                       | CVPR 2019   |

#### Multifaceted Benchmarks

| Benchmark                                                | Task                | Data    | Paper | Preprint | Publication | Affiliation |
| -------------------------------------------------------- | ------------------- | ------- | ----- | -------- | ----------- | ----------- |
| [Video-Bench](https://github.com/PKU-YuanGroup/Video-Bench) | MC (general domain) | mixture |       |          |             |             |

## Metrics

- [Common Metrics on Video Quality](https://github.com/JunyaoHu/common_metrics_on_video_quality), You can easily calculate FVD, PSNR, SSIM, LPIPS for evaluating the quality of generated or predicted videos.

## Projects & Tools

- [VideoDB](https://github.com/video-db/StreamRAG), It enables developers to: 1) Upload multiple videos to create a library or collection; 2) Search across these videos and get real-time video responses or compilations; 3) Publish your searchable collection on the ChatGPT store; 4) Receive summarized text answers (RAG); 5) Gain key insights from specific videos (e.g. "Top points from episode 31").
- [video2dataset](https://github.com/bryant1410/video2dataset), Easily create large video dataset from video urls. Can download and package 10M videos in 12h on a single 16 core machine.
- [Match cutting](https://github.com/Netflix/matchcut), A match cut is a transition between a pair of shots that uses similar framing, composition, or action to fluidly bring the viewer from one scene to the next
- [Awesome-Video-Object-Segmentation](https://github.com/gaomingqi/Awesome-Video-Object-Segmentation), A curated list of video object segmentation (vos) papers, datasets, and projects.
- [pytube](https://github.com/pytube/pytube), A lightweight, dependency-free Python library (and command-line utility) for downloading YouTube Videos.
- [movienet-tools](https://github.com/movienet/movienet-tools/blob/master/docs/GETTING_STARTED.md), Movie toolbox provides many basic tools and functions for the researches on movie understanding, with which you can get started with your research easily.
- [PySceneDetect](https://github.com/Breakthrough/PySceneDetect), Video Scene Cut Detection and Analysis Tool

# Video Generation

## Reading List

Survey

- (2023-10) A Survey on Video Diffusion Models  [paper](https://arxiv.org/abs/2310.10647)  [repo](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models)

Reading List

| Paper                                                                               | Base Structure         | Data           | Code                                                           | Publication | Preprint                                                                          | Affiliation     |
| ----------------------------------------------------------------------------------- | ---------------------- | -------------- | -------------------------------------------------------------- | ----------- | --------------------------------------------------------------------------------- | --------------- |
| Video generation models as world simulators                                         | Transformer            | -              | -                                                              | -           | [2402.blog](https://openai.com/research/video-generation-models-as-world-simulators) | OpenAI          |
| Vlogger: Make Your Dream A Vlog                                                     | *Diffusion*          |                | [Vlogger](https://github.com/zhuangshaobin/Vlogger)               |             | [2401.09414](https://arxiv.org/abs/2401.09414)                                       | Shanghai AI Lab |
| FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis     | *ControlNet*         |                | [FlowVid](https://github.com/Jeff-LiangF/FlowVid)                 |             | [2312.17681](https://arxiv.org/abs/2312.17681)                                       | Meta            |
| SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction | *Diffusion*          |                | [SEINE](https://github.com/Vchitect/SEINE)                        |             | [2310.20700](https://arxiv.org/abs/2310.20700)                                       | Shanghai AI Lab |
| MotionDirector: Motion Customization of Text-to-Video Diffusion Models              | *Diffusion*          |                | [MotionDirector](https://github.com/showlab/MotionDirector)       |             | [2310.08465](https://arxiv.org/abs/2310.08465)                                       | NUS             |
| VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning   | GPT4 +*UNet*         |                | [VideoDirectorGPT](https://github.com/HL-hanlin/VideoDirectorGPT) |             | [2309.15091](https://arxiv.org/abs/2309.15091)                                       | UNC             |
| CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers     | Transformer +*VQVAE* | self-construct | [CogVideo](https://github.com/THUDM/CogVideo)                     |             | [2205.15868](https://arxiv.org/abs/2205.15868)                                       | THU             |

## Metrics

- [T2VScore](https://github.com/showlab/T2VScore), T2VScore: Towards A Better Metric for Text-to-Video Generation

## Projects

- [Open Chat Video Editor](https://github.com/SCUTlihaoyu/open-chat-video-editor), Open source short video automatic generation tool
