# Image

Table of Contents

- [Image Understanding](#image-understanding)
  - [Reading List](#reading-list)
  - [Dataset](#dataset)
  - [Benchmarks](#benchmarks)
- [Image Generation](#image-generation)
  - [Reading List](#reading-list-1)

## Image Understanding

### Reading List

*NOTEs: INST=Instruction, FT=Finetune, PT=Pretraining*

| Paper                                                                                                                                                                     | Base Language Model         | Framework                     | Data                                                                                                                                        | Code                                                                                          | Publication | Preprint                                    | Affiliation      |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------- | ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ----------- | ------------------------------------------- | ---------------- |
| Visual Instruction Tuning with Polite Flamingo                                                                                                                            | Flamingo                    | FT + (rewrite instruction)    | PF-1M, LLaVA-instruciton-177k                                                                                                               | [Polite Flamingo](https://github.com/ChenDelong1999/polite_flamingo)                             |             | [2307.01003](https://arxiv.org/abs/2307.01003) | Xiaobing         |
| LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding                                                                                             | Vicuna-13B                  | FT + MM-INST                  | self-construct (text-rich image)                                                                                                            | [LLaVAR](https://github.com/SALT-NLP/LLaVAR)                                                     |             | [2306.17107](https://arxiv.org/abs/2306.17107) | Gatech           |
| Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic                                                                                                           | Vicuna-7B/13B               | FT + MM-INST                  | self-constuct (referential dialogue)                                                                                                        | [Shikra](https://github.com/shikras/shikra)                                                      |             | [2306.15195](https://arxiv.org/abs/2306.15195) | SenseTime        |
| KOSMOS-2: Grounding Multimodal Large Language Models to the World                                                                                                        | Magneto                     | PT + obj                      | [Grit](https://github.com/microsoft/unilm/tree/master/kosmos-2#grit-large-scale-training-corpus-of-grounded-image-text-pairs) (90M images)    | [Kosmos-2](https://github.com/microsoft/unilm/tree/master/kosmos-2)                              |             | [2306.14824](https://arxiv.org/abs/2306.14824) | Microsoft        |
| Aligning Large Multi-Modal Model with Robust Instruction Tuning                                                                                                          | Vicuna (MiniGPT4-like)      | FT + MM-INST                  | [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction#visual-instruction-data-lrv-instruction) (150K INST, robust), GAVIE (evaluate) | [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction)                                  |             | [2306.14565](https://arxiv.org/abs/2306.14565) | UMD              |
| LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark                                                                                  | Vicuna-7B/13B               | FT + MM-INST                  | [LAMM-Dataset](https://github.com/OpenLAMM/LAMM#lamm-dataset) (186K INST), [LAMM-Benchmark](https://github.com/OpenLAMM/LAMM#lamm-benchmark)     | [LAMM](https://github.com/OpenLAMM/LAMM)                                                         |             | [2306.06687](https://arxiv.org/abs/2306.06687) | Shanghai AI Lab  |
| ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst                                                                                          | Vicuna-13B                  | FT + MM-INST                  |                                                                                                                                             | [ChatBridge](https://github.com/joez17/ChatBridge)                                               |             | [2305.16103](https://arxiv.org/abs/2305.16103) | CAS              |
| Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models                                                                                   | LLaMA-7B/13B                | FT adapter + MM-INST         | [self-construc](https://github.com/luogen1996/LaVIN#data-preparation) (INST)                                                                  | [LaVIN](https://github.com/luogen1996/LaVIN)                                                     |             | [2305.15023](https://arxiv.org/abs/2305.15023) | Xiamen Univ.     |
| DetGPT: Detect What You Need via Reasoning                                                                                                                                | Robin, Vicuna               | FT + MM-INST + detector       | self-construct                                                                                                                              | [DetGPT](https://github.com/OptimalScale/DetGPT)                                                 |             | [2305.14167](https://arxiv.org/abs/2305.14167) | HKUST            |
| VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks                                                                                    | Alpaca                      |                               |                                                                                                                                             | [VisionLLM](https://github.com/OpenGVLab/VisionLLM)                                              |             | [2305.11175](https://arxiv.org/abs/2305.11175) | Shanghai AI Lab. |
| InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning                                                                                      | Vicuna                      |                               |                                                                                                                                             | [InstructBLIP](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)              |             | [2305.06500](https://arxiv.org/abs/2305.06500) | Salesforce       |
| MultiModal-GPT: A Vision and Language Model for Dialogue with Humans                                                                                                     | Flamingo                    | FT + MM-INST, LoRA            | mixture                                                                                                                                     | [Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT)                                   |             | [2305.04790](https://arxiv.org/abs/2305.04790) | NUS              |
| Otter: A Multi-Modal Model with In-Context Instruction Tuning                                                                                                             | Flamingo                    |                               |                                                                                                                                             | [Otter](https://github.com/Luodian/Otter)                                                        |             | [2305.03726](https://arxiv.org/abs/2305.03726) | NTU              |
| X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages                                                                     | ChatGPT                     |                               |                                                                                                                                             | [X-LLM](https://github.com/phellonchen/X-LLM)                                                    |             | [2305.04160](https://arxiv.org/abs/2305.04160) | CAS              |
| LMEye: An Interactive Perception Network for Large Language Models                                                                                                       | OPT,Bloomz,BLIP2            | PT, FT + MM-INST             | self-construct                                                                                                                              | [LingCloud](https://github.com/YunxinLi/LingCloud)                                               |             | [2305.03701](https://arxiv.org/abs/2305.03701) | HIT              |
| Multimodal Procedural Planning via Dual Text-Image Prompting                                                                                                              | OFA, BLIP, GPT3             |                               |                                                                                                                                             | [TIP](https://github.com/YujieLu10/TIP)                                                          |             | [2305.01795](https://arxiv.org/abs/2305.01795) | UCSB             |
| Transfer Visual Prompt Generator across LLMs                                                                                                                              | FlanT5, OPT                 | projecter + transfer strategy |                                                                                                                                             | [VPGTrans](https://github.com/VPGTrans/VPGTrans)                                                 |             | [2305.01278](https://arxiv.org/abs/2305.01278) | CUHK             |
| LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model                                                                                                            | LLaMA                       |                               |                                                                                                                                             | [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)                                   |             | [2304.15010](https://arxiv.org/abs/2304.15010) | Shanghai AI Lab. |
| mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality,[mPLUG](https://aclanthology.org/2022.emnlp-main.488/), [mPLUG-2](https://arxiv.org/abs/2302.00402) | LLaMA                       |                               |                                                                                                                                             | [mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl)                                                 |             | [2304.14178](https://arxiv.org/abs/2304.14178) | DAMO Academy     |
| MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models                                                                                    | Vicunna                     |                               |                                                                                                                                             | [MiniGPT4](https://github.com/Vision-CAIR/MiniGPT-4)                                             |             | [2304.10592](https://arxiv.org/abs/2304.10592) | KAUST            |
| Visual Instruction Tuning                                                                                                                                                 | LLaMA                       | full-param. + INST tuning     | [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) (150K INST by GPT4)                                     | [LLaVA](https://github.com/haotian-liu/LLaVA)                                                    |             | [2304.08485](https://arxiv.org/abs/2304.08485) | Microsoft        |
| MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action                                                                                                           | ChatGPT                     |                               |                                                                                                                                             | [MM-REACT](https://github.com/microsoft/MM-REACT)                                                |             | [2303.11381](https://arxiv.org/abs/2303.11381) | Microsoft        |
| ViperGPT: Visual Inference via Python Execution for Reasoning                                                                                                             | Codex                       |                               |                                                                                                                                             | [ViperGPT](https://github.com/cvlab-columbia/viper)                                              |             | [2303.08128](https://arxiv.org/abs/2303.08128) | Columbia         |
| Scaling Vision-Language Models with Sparse Mixture of Experts                                                                                                             | (MOE + Scaling)             |                               |                                                                                                                                             |                                                                                               |             | [2303.07226](https://arxiv.org/abs/2303.07226) | Microsoft        |
| ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions                                                                                  | ChatGPT, Flan-T5 (BLIP2)    |                               |                                                                                                                                             | [ChatCaptioner](https://github.com/Vision-CAIR/ChatCaptioner)                                    |             | [2303.06594](https://arxiv.org/abs/2303.06594) | KAUST            |
| Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models                                                                                                | ChatGPT                     |                               |                                                                                                                                             | [Visual ChatGPT](https://github.com/microsoft/visual-chatgpt)                                    |             | [2303.04671](https://arxiv.org/abs/2303.04671) | Microsoft        |
| PaLM-E: An Embodied Multimodal Language Model                                                                                                                             | PaLM                        |                               |                                                                                                                                             |                                                                                               |             | [2303.03378](https://arxiv.org/abs/2303.03378) | Google           |
| Prismer: A Vision-Language Model with An Ensemble of Experts                                                                                                              | RoBERTa, OPT, BLOOM         |                               |                                                                                                                                             | [Prismer](https://github.com/NVlabs/prismer)                                                     |             | [2303.02506](https://arxiv.org/abs/2303.02506) | Nvidia           |
| Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering                                                                      | GPT3                        |                               |                                                                                                                                             | [Prophet](https://github.com/MILVLG/prophet)                                                     | CVPR2023    | [2303.01903](https://arxiv.org/abs/2303.01903) | HDU              |
| Language Is Not All You Need: Aligning Perception with Language Models                                                                                                    | Magneto                     |                               |                                                                                                                                             | [KOSMOS-1](https://github.com/microsoft/unilm)                                                   |             | [2302.14045](https://arxiv.org/abs/2302.14045) | Microsoft        |
| Scaling Vision Transformers to 22 Billion Parameters                                                                                                                      | (CLIP + Scaling)            |                               |                                                                                                                                             |                                                                                               |             | [2302.05442](https://arxiv.org/abs/2302.05442) | Google           |
| Multimodal Chain-of-Thought Reasoning in Language Models                                                                                                                  | T5                          |                               |                                                                                                                                             | [MM-COT](https://github.com/amazon-science/mm-cot)                                               |             | [2302.00923](https://arxiv.org/abs/2302.00923) | Amazon           |
| Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Caption                                                                                    | RETRO                       |                               |                                                                                                                                             |                                                                                               |             | [2302.04858](https://arxiv.org/abs/2302.04858) | Nvidia           |
| BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models                                                                    | Flan-T5 / qformer           |                               |                                                                                                                                             | [BLIP2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)                            | ICML2023    | [2301.12597](https://arxiv.org/abs/2301.12597) | Salesforce       |
| See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning                                                        | OPT                         |                               |                                                                                                                                             |                                                                                               |             | [2301.05226](https://arxiv.org/abs/2301.05226) | MIT-IBM          |
| Generalized Decoding for Pixel, Image, and Language                                                                                                                       | GPT3                        |                               |                                                                                                                                             | [X-GPT](https://github.com/microsoft/X-Decoder/tree/xgpt)                                        |             | [2212.11270](https://arxiv.org/abs/2212.11270) | Microsoft        |
| From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models                                                                     | OPT                         |                               |                                                                                                                                             | [Img2LLM](https://github.com/salesforce/LAVIS/tree/main/projects/img2prompt-vqa)                 | CVPR2023    | [2212.10846](https://arxiv.org/abs/2212.10846) | Salesforce       |
| Language Models are General-Purpose Interfaces                                                                                                                            | DeepNorm                    |                               |                                                                                                                                             | [METALM](https://github.com/microsoft/unilm)                                                     |             | [2206.06336](https://arxiv.org/abs/2206.06336) | Microsoft        |
| Language Models Can See: Plugging Visual Controls in Text Generation                                                                                                      | GPT2                        |                               |                                                                                                                                             | [MAGIC](https://github.com/yxuansu/MAGIC)                                                        |             | [2205.02655](https://arxiv.org/abs/2205.02655) | Tencent          |
| Flamingo: a Visual Language Model for Few-Shot Learning                                                                                                                   | Chinchilla / adapter        |                               |                                                                                                                                             | [Flamingo](https://github.com/lucidrains/flamingo-pytorch)                                       | NIPS 2022   | [2204.14198](https://arxiv.org/abs/2204.14198) | DeepMind         |
| An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA                                                                                                              | GPT3                        |                               |                                                                                                                                             | [PICa](https://github.com/microsoft/PICa)                                                        | AAAI2022    | [2109.05014](https://arxiv.org/abs/2109.05014) | Microsoft        |
| Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language                                                                                                   | GPT3, RoBERTa               |                               |                                                                                                                                             | [Socratic Models](https://github.com/google-research/google-research/tree/master/socraticmodels) | ICLR 2023   | [2204.00598](https://arxiv.org/abs/2204.00598) | Google           |
| Learning Transferable Visual Models From Natural Language Supervision                                                                                                     | Bert / contrastive learning |                               |                                                                                                                                             | [CLIP](https://github.com/openai/CLIP)                                                           | ICML 2021   | [2103.00020](https://arxiv.org/abs/2103.00020) | OpenAI           |

### Datasets

- [DataComp](https://github.com/mlfoundations/datacomp)
- [M3IT](https://huggingface.co/datasets/MMInstruction/M3IT)
- [MulitInstruct](https://github.com/VT-NLP/MultiInstruct)

### Benchmarks

| Benchmark                                                 | Task | Data                                                                                 | Paper                                                                                   | Preprint                                    | Publication | Affiliation |
| --------------------------------------------------------- | ---- | ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- | ------------------------------------------- | ----------- | ----------- |
| [INFOSEEK](https://open-vision-language.github.io/infoseek/) | VQA  | [OVEN (open domain image)](https://open-vision-language.github.io/oven/) + Human Anno. | Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions? | [2302.11713](https://arxiv.org/abs/2302.11713) |             | Google      |

## Image Generation

### Reading List

*Include some insightful works except for LLM*

| Paper                                                | Base Language Model | Code                                                         | Publication | Preprint                                    | Affiliation     |
| ---------------------------------------------------- | ------------------- | ------------------------------------------------------------ | ----------- | ------------------------------------------- | --------------- |
| CoDi: Any-to-Any Generation via Composable Diffusion | *Diffusion*       | [CoDi](https://github.com/microsoft/i-Code/tree/main/i-Code-V3) |             | [2305.11846](https://arxiv.org/abs/2305.11846) | Microsoft & UNC |
| ImageBind: One Embedding Space To Bind Them All      | CLIP                | [ImageBind](https://github.com/facebookresearch/ImageBind)      |             | [2305.05665](https://arxiv.org/abs/2305.05665) | Meta            |
| Denoising Diffusion Probabilistic Models             | *Diffusion*       | [diffusion](https://github.com/hojonathanho/diffusion)          |             | [2006.11239](https://arxiv.org/abs/2006.11239) | UCB             |
