# Image

Table of Contents

- [Image Understanding](#image-understanding)
  - [Reading List](#reading-list)
  - [Datasets &amp; Benchmarks](#datasets--benchmarks)
- [Image Generation](#image-generation)
  - [Reading List](#reading-list-1)
- [Open Source Projects](#open-source-projects)

## Image Understanding

### Reading List

*NOTEs: INST=Instruction, FT=Finetune, PT=Pretraining, ICL=In Context Learning, ZS=ZeroShot, FS=FewShot, RTr=Retrieval*

| Paper                                                                                                                                                                     | Base Language Model             | Framework                                | Data                                                                                                                                        | Code                                                                                          | Publication                    | Preprint                                                                           | Affiliation       |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------- | ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------ | ---------------------------------------------------------------------------------- | ----------------- |
| COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training                                                                                             | gated cross-attn + latent array | PT + FT                                  | mixture                                                                                                                                     | [cosmo](https://github.com/showlab/cosmo)                                                        |                                | [2401.00849](https://arxiv.org/abs/2401.00849)                                        | NUS               |
| Tracking with Human-Intent Reasoning                                                                                                                                      | LLaMA (LLaVA)                   | PT+FT                                    | mixture                                                                                                                                     | [TrackGPT](https://github.com/jiawen-zhu/TrackGPT)                                               |                                | [2312.17448](https://arxiv.org/abs/2312.17448)                                        | Alibaba           |
| VCoder: Versatile Vision Encoders for Multimodal Large Language Models                                                                                                    | LLaMA (LLaVA-1.5)               | FT (depth encoder + segment encoder)     | COCO Segmentation Text ([COST](https://huggingface.co/datasets/shi-labs/COST))                                                                 | [VCoder](https://github.com/SHI-Labs/VCoder)                                                     |                                | [2312.14233](https://arxiv.org/abs/2312.14233)                                        | Gatech            |
| Osprey: Pixel Understanding with Visual Instruction Tuning                                                                                                                | Vicuna                          | PT+FT                                    | mixture                                                                                                                                     | [Osprey](https://github.com/CircleRadon/Osprey)                                                  |                                | [2312.10032](https://arxiv.org/abs/2312.10032)                                        | ZJU               |
| Tokenize Anything via Prompting                                                                                                                                           | SAM                             | PT                                       | mixture (mainly SA-1B)                                                                                                                      | [tokenize-anything](https://github.com/baaivision/tokenize-anything)                             |                                | [2312.09128](https://arxiv.org/abs/2312.09128)                                        | BAAI              |
| Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens                                                                                                  | LLaVA                           | INST                                     | video-chatgpt                                                                                                                               | [Vista-LLaMA (web)](https://jinxxian.github.io/Vista-LLaMA/)                                     |                                | [2312.08870](https://arxiv.org/abs/2312.08870)                                        | ByteDance         |
| Gemini: A Family of Highly Capable Multimodal Models                                                                                                                      | Transformer-Decoder             | FT (language decoder + image decoder)    | ?                                                                                                                                           | ?                                                                                             | -                              | [2312.blog](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) | Google            |
| LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models                                                                                                        | LLaMA (LLaVA)                   | FT(FT grounding model + INSTFT)          | RefCOCO + Flickr30K + LLaVA                                                                                                                 | [LLaVA-Grounding](https://github.com/UX-Decoder/LLaVA-Grounding)                                 |                                | [2312.02949](https://arxiv.org/abs/2312.02949)                                        | MSR               |
| Making Large Multimodal Models Understand Arbitrary Visual Prompts                                                                                                        | LLaMA (LLaVA)                   | PT+INSTFT                                | BLIP + LLaVA-1.5                                                                                                                            | [ViP-LLaVA](https://github.com/mu-cai/ViP-LLaVA)                                                 |                                | [2312.00784](https://arxiv.org/abs/2312.00784)                                        | Wisconsin-Madison |
| Sequential Modeling Enables Scalable Learning for Large Vision Models                                                                                                     | LLaMA                           | PT (**Visual Tokenizer**)          | mixture (430B visual tokens, 50 dataset, mainly from LAION)                                                                                 | [LVM](https://github.com/ytongbai/LVM)                                                           |                                | [2312.00785](https://arxiv.org/abs/2312.00785)                                        | UCB               |
| GLaMM: Pixel Grounding Large Multimodal Model                                                                                                                             | Vicuna-1.5                      | FT                                       | self-construct ([grounding-anything-dataset](https://github.com/mbzuai-oryx/groundingLMM#-grounding-anything-dataset-grand))                   | [GLaMM](https://github.com/mbzuai-oryx/groundingLMM)                                             |                                | [2311.03356](https://arxiv.org/abs/2311.03356)                                        | MBZU              |
| CogVLM: Visual Expert For Large Language Models                                                                                                                           | Vicuna                          | PT + FT                                  | self-construct + mixture                                                                                                                    | [CogVLM](https://github.com/THUDM/CogVLM)                                                        |                                | [2309.github](https://github.com/THUDM/CogVLM/blob/main/assets/cogvlm-paper.pdf)      | Zhipu AI          |
| GPT-4V(ision) System Card                                                                                                                                                 | GPT4                            | ？                                       | ？                                                                                                                                          | ？                                                                                            | -                              | [2309.blog](https://cdn.openai.com/papers/GPTV_System_Card.pdf)                       | OpenAI            |
| Demystifying CLIP Data                                                                                                                                                    | CLIP                            | PT                                       | [curated &amp; transparent CLIP dataset](https://github.com/facebookresearch/MetaCLIP/blob/main/metadata.json)                                 | [MetaCLIP](https://github.com/facebookresearch/MetaCLIP)                                         |                                | [2309.16671](https://arxiv.org/abs/2309.16671)                                        | Meta              |
| InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition                                                                   | InternLM                        | PT + FT                                  | mixture                                                                                                                                     | [InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer)                             |                                | [2309.15112](https://arxiv.org/abs/2309.15112)                                        | Shanghai AI Lab.  |
| DreamLLM: Synergistic Multimodal Comprehension and Creation                                                                                                               | LLaMA                           | PT + FT                                  | mixture                                                                                                                                     | [DreamLLM](https://github.com/RunpeiDong/DreamLLM)                                               |                                | [2309.11499](https://arxiv.org/abs/2309.11499)                                        | MEGVII            |
| LaVIT: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization                                                                               | LLaMA                           | PT + FT (**Visual Tokenizer**)     | mixture                                                                                                                                     | [LaVIT](https://github.com/jy0205/LaVIT)                                                         |                                | [2309.04669](https://arxiv.org/abs/2309.04669)                                        | Kuaishou          |
| Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond                                                                      | QWen                            | PT + FT                                  |                                                                                                                                             | [Qwen-VL](https://github.com/QwenLM/Qwen-VL)                                                     |                                | [2308.12966](https://arxiv.org/abs/2308.12966)                                        | Alibaba           |
| BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions                                                                                          | Vicuna-7B/Flan-t5-xxl           | FT                                       | same as InstructBLIP                                                                                                                        | [BLIVA](https://github.com/mlpc-ucsd/BLIVA)                                                      |                                | [2308.09936](https://arxiv.org/abs/2308.09936)                                        | UCSD              |
| LISA: Reasoning Segmentation via Large Language Model                                                                                                                     | LLaMA                           | PT                                       | mixture                                                                                                                                     | [LISA](https://github.com/dvlab-research/LISA)                                                   |                                | [2308.00692](https://arxiv.org/abs/2308.00692)                                        | SmartMore         |
| Generative Pretraining in Multimodality, v2                                                                                                                               | LLaMA，*Diffusion*            | PT, Visual Decoder                       | mixture                                                                                                                                     | [Emu](https://github.com/baaivision/Emu)                                                         |                                | [2307.05222](https://arxiv.org/abs/2307.05222)                                        | BAAI              |
| What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?                                                                                              | Vicuna-7b                       | PT + FT                                  | mixture (emperical)                                                                                                                         | [lynx](https://github.com/bytedance/lynx-llm)                                                    |                                | [2307.02469](https://arxiv.org/abs/2307.02469)                                        | Bytedance         |
| Visual Instruction Tuning with Polite Flamingo                                                                                                                            | Flamingo                        | FT + (rewrite instruction)               | PF-1M, LLaVA-instruciton-177k                                                                                                               | [Polite Flamingo](https://github.com/ChenDelong1999/polite_flamingo)                             |                                | [2307.01003](https://arxiv.org/abs/2307.01003)                                        | Xiaobing          |
| LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding                                                                                             | Vicuna-13B                      | FT + MM-INST                             | self-construct (text-rich image)                                                                                                            | [LLaVAR](https://github.com/SALT-NLP/LLaVAR)                                                     |                                | [2306.17107](https://arxiv.org/abs/2306.17107)                                        | Gatech            |
| Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic                                                                                                           | Vicuna-7B/13B                   | FT + MM-INST                             | self-constuct (referential dialogue)                                                                                                        | [Shikra](https://github.com/shikras/shikra)                                                      |                                | [2306.15195](https://arxiv.org/abs/2306.15195)                                        | SenseTime         |
| KOSMOS-2: Grounding Multimodal Large Language Models to the World                                                                                                        | Magneto                         | PT + obj                                 | [Grit](https://github.com/microsoft/unilm/tree/master/kosmos-2#grit-large-scale-training-corpus-of-grounded-image-text-pairs) (90M images)    | [Kosmos-2](https://github.com/microsoft/unilm/tree/master/kosmos-2)                              |                                | [2306.14824](https://arxiv.org/abs/2306.14824)                                        | Microsoft         |
| Aligning Large Multi-Modal Model with Robust Instruction Tuning                                                                                                          | Vicuna (MiniGPT4-like)          | FT + MM-INST                             | [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction#visual-instruction-data-lrv-instruction) (150K INST, robust), GAVIE (evaluate) | [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction)                                  |                                | [2306.14565](https://arxiv.org/abs/2306.14565)                                        | UMD               |
| LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark                                                                                  | Vicuna-7B/13B                   | FT + MM-INST                             | [LAMM-Dataset](https://github.com/OpenLAMM/LAMM#lamm-dataset) (186K INST), [LAMM-Benchmark](https://github.com/OpenLAMM/LAMM#lamm-benchmark)     | [LAMM](https://github.com/OpenLAMM/LAMM)                                                         |                                | [2306.06687](https://arxiv.org/abs/2306.06687)                                        | Shanghai AI Lab   |
| Improving CLIP Training with Language Rewrites                                                                                                                            | CLIP + ChatGPT                  | FT + Data-aug                            | mixture                                                                                                                                     | [LaCLIP](https://github.com/LijieFan/LaCLIP)                                                     |                                | [2305.20088](https://arxiv.org/abs/2305.20088)                                        | Google            |
| ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst                                                                                          | Vicuna-13B                      | FT + MM-INST                             |                                                                                                                                             | [ChatBridge](https://github.com/joez17/ChatBridge)                                               |                                | [2305.16103](https://arxiv.org/abs/2305.16103)                                        | CAS               |
| Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models                                                                                   | LLaMA-7B/13B                    | FT adapter + MM-INST                    | [self-construc](https://github.com/luogen1996/LaVIN#data-preparation) (INST)                                                                  | [LaVIN](https://github.com/luogen1996/LaVIN)                                                     |                                | [2305.15023](https://arxiv.org/abs/2305.15023)                                        | Xiamen Univ.      |
| IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models                                                                                 | ChatGPT                         | iterative, compositional (que, ans, rea) | ZS                                                                                                                                          | [IdeaGPT](https://github.com/Hxyou/IdealGPT)                                                     |                                | [2305.14985](https://arxiv.org/abs/2305.14985)                                        | Columbia          |
| DetGPT: Detect What You Need via Reasoning                                                                                                                                | Robin, Vicuna                   | FT + MM-INST + detector                  | self-construct                                                                                                                              | [DetGPT](https://github.com/OptimalScale/DetGPT)                                                 |                                | [2305.14167](https://arxiv.org/abs/2305.14167)                                        | HKUST             |
| VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks                                                                                    | Alpaca                          |                                          |                                                                                                                                             | [VisionLLM](https://github.com/OpenGVLab/VisionLLM)                                              |                                | [2305.11175](https://arxiv.org/abs/2305.11175)                                        | Shanghai AI Lab.  |
| InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning                                                                                      | Vicuna                          |                                          |                                                                                                                                             | [InstructBLIP](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)              |                                | [2305.06500](https://arxiv.org/abs/2305.06500)                                        | Salesforce        |
| MultiModal-GPT: A Vision and Language Model for Dialogue with Humans                                                                                                     | Flamingo                        | FT + MM-INST, LoRA                       | mixture                                                                                                                                     | [Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT)                                   |                                | [2305.04790](https://arxiv.org/abs/2305.04790)                                        | NUS               |
| Otter: A Multi-Modal Model with In-Context Instruction Tuning                                                                                                             | Flamingo                        |                                          |                                                                                                                                             | [Otter](https://github.com/Luodian/Otter)                                                        |                                | [2305.03726](https://arxiv.org/abs/2305.03726)                                        | NTU               |
| X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages                                                                     | ChatGPT                         |                                          |                                                                                                                                             | [X-LLM](https://github.com/phellonchen/X-LLM)                                                    |                                | [2305.04160](https://arxiv.org/abs/2305.04160)                                        | CAS               |
| LMEye: An Interactive Perception Network for Large Language Models                                                                                                       | OPT,Bloomz,BLIP2                | PT, FT + MM-INST                        | self-construct                                                                                                                              | [LingCloud](https://github.com/YunxinLi/LingCloud)                                               |                                | [2305.03701](https://arxiv.org/abs/2305.03701)                                        | HIT               |
| Caption anything: Interactive image description with diverse multimodal controls                                                                                          | BLIP2, ChatGPT                  | ZS                                       |                                                                                                                                             | [Caption Anything](https://github.com/ttengwang/Caption-Anything)                                |                                | [2305.02677](https://arxiv.org/abs/2305.02677)                                        | SUSTech           |
| Multimodal Procedural Planning via Dual Text-Image Prompting                                                                                                              | OFA, BLIP, GPT3                 |                                          |                                                                                                                                             | [TIP](https://github.com/YujieLu10/TIP)                                                          |                                | [2305.01795](https://arxiv.org/abs/2305.01795)                                        | UCSB              |
| Transfer Visual Prompt Generator across LLMs                                                                                                                              | FlanT5, OPT                     | projecter + transfer strategy            |                                                                                                                                             | [VPGTrans](https://github.com/VPGTrans/VPGTrans)                                                 |                                | [2305.01278](https://arxiv.org/abs/2305.01278)                                        | CUHK              |
| LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model                                                                                                            | LLaMA                           |                                          |                                                                                                                                             | [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)                                   |                                | [2304.15010](https://arxiv.org/abs/2304.15010)                                        | Shanghai AI Lab.  |
| mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality,[mPLUG](https://aclanthology.org/2022.emnlp-main.488/), [mPLUG-2](https://arxiv.org/abs/2302.00402) | LLaMA                           |                                          |                                                                                                                                             | [mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl)                                                 |                                | [2304.14178](https://arxiv.org/abs/2304.14178)                                        | DAMO Academy      |
| MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models                                                                                    | Vicunna                         |                                          |                                                                                                                                             | [MiniGPT4](https://github.com/Vision-CAIR/MiniGPT-4)                                             |                                | [2304.10592](https://arxiv.org/abs/2304.10592)                                        | KAUST             |
| Visual Instruction Tuning                                                                                                                                                 | LLaMA                           | full-param. + INST tuning                | [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) (150K INST by GPT4)                                     | [LLaVA](https://github.com/haotian-liu/LLaVA)                                                    |                                | [2304.08485](https://arxiv.org/abs/2304.08485)                                        | Microsoft         |
| Chain of Thought Prompt Tuning in Vision Language Models                                                                                                                  | -                               | Visual CoT                               | -                                                                                                                                           |                                                                                               |                                | [2304.07919](https://arxiv.org/abs/2304.07919)                                        | PKU               |
| MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action                                                                                                           | ChatGPT                         |                                          |                                                                                                                                             | [MM-REACT](https://github.com/microsoft/MM-REACT)                                                |                                | [2303.11381](https://arxiv.org/abs/2303.11381)                                        | Microsoft         |
| ViperGPT: Visual Inference via Python Execution for Reasoning                                                                                                             | Codex                           |                                          |                                                                                                                                             | [ViperGPT](https://github.com/cvlab-columbia/viper)                                              | ICCV 2023                      | [2303.08128](https://arxiv.org/abs/2303.08128)                                        | Columbia          |
| Scaling Vision-Language Models with Sparse Mixture of Experts                                                                                                             | (MOE + Scaling)                 |                                          |                                                                                                                                             |                                                                                               |                                | [2303.07226](https://arxiv.org/abs/2303.07226)                                        | Microsoft         |
| ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions                                                                                  | ChatGPT, Flan-T5 (BLIP2)        |                                          |                                                                                                                                             | [ChatCaptioner](https://github.com/Vision-CAIR/ChatCaptioner)                                    |                                | [2303.06594](https://arxiv.org/abs/2303.06594)                                        | KAUST             |
| Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models                                                                                                | ChatGPT                         |                                          |                                                                                                                                             | [Visual ChatGPT](https://github.com/microsoft/visual-chatgpt)                                    |                                | [2303.04671](https://arxiv.org/abs/2303.04671)                                        | Microsoft         |
| PaLM-E: An Embodied Multimodal Language Model                                                                                                                             | PaLM                            |                                          |                                                                                                                                             |                                                                                               |                                | [2303.03378](https://arxiv.org/abs/2303.03378)                                        | Google            |
| Prismer: A Vision-Language Model with An Ensemble of Experts                                                                                                              | RoBERTa, OPT, BLOOM             |                                          |                                                                                                                                             | [Prismer](https://github.com/NVlabs/prismer)                                                     |                                | [2303.02506](https://arxiv.org/abs/2303.02506)                                        | Nvidia            |
| Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners                                                                                 | GPT3, CLIP, DINO, DALLE         |                                          | FS, evaluate: img-cls                                                                                                                       | [CaFo](https://github.com/OpenGVLab/CaFo)                                                        | CVPR 2023                      | [2303.02151](https://arxiv.org/abs/2303.02151)                                        | CAS               |
| Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering                                                                      | GPT3                            | RTr+candidates, ICL                      | [evaluate](https://github.com/MILVLG/prophet#data-preparation): OKVQA, A-OKVQA                                                                 | [Prophet](https://github.com/MILVLG/prophet)                                                     | CVPR 2023                      | [2303.01903](https://arxiv.org/abs/2303.01903)                                        | HDU               |
| Language Is Not All You Need: Aligning Perception with Language Models                                                                                                    | Magneto                         |                                          |                                                                                                                                             | [KOSMOS-1](https://github.com/microsoft/unilm)                                                   |                                | [2302.14045](https://arxiv.org/abs/2302.14045)                                        | Microsoft         |
| Scaling Vision Transformers to 22 Billion Parameters                                                                                                                      | (CLIP + Scaling)                |                                          |                                                                                                                                             |                                                                                               |                                | [2302.05442](https://arxiv.org/abs/2302.05442)                                        | Google            |
| Multimodal Chain-of-Thought Reasoning in Language Models                                                                                                                  | T5                              | FT + MM-CoT                              |                                                                                                                                             | [MM-COT](https://github.com/amazon-science/mm-cot)                                               |                                | [2302.00923](https://arxiv.org/abs/2302.00923)                                        | Amazon            |
| Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Caption                                                                                    | RETRO                           |                                          |                                                                                                                                             |                                                                                               |                                | [2302.04858](https://arxiv.org/abs/2302.04858)                                        | Nvidia            |
| BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models                                                                    | Flan-T5 / qformer               |                                          |                                                                                                                                             | [BLIP2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)                            | ICML 2023                      | [2301.12597](https://arxiv.org/abs/2301.12597)                                        | Salesforce        |
| See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning                                                        | OPT                             |                                          |                                                                                                                                             |                                                                                               |                                | [2301.05226](https://arxiv.org/abs/2301.05226)                                        | MIT-IBM           |
| Generalized Decoding for Pixel, Image, and Language                                                                                                                       | GPT3                            |                                          |                                                                                                                                             | [X-GPT](https://github.com/microsoft/X-Decoder/tree/xgpt)                                        |                                | [2212.11270](https://arxiv.org/abs/2212.11270)                                        | Microsoft         |
| From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models                                                                     | OPT                             |                                          |                                                                                                                                             | [Img2LLM](https://github.com/salesforce/LAVIS/tree/main/projects/img2prompt-vqa)                 | CVPR 2023                      | [2212.10846](https://arxiv.org/abs/2212.10846)                                        | Salesforce        |
| Visual Programming: Compositional visual reasoning without training                                                                                                       | GPT3                            | Compositional/Tool-Learning              |                                                                                                                                             | [VisProg](https://github.com/allenai/visprog)                                                    | CVPR 2023<br /> *best paper* | [2211.11559](https://arxiv.org/abs/2211.11559)                                        | AI2               |
| Language Models are General-Purpose Interfaces                                                                                                                            | DeepNorm                        | Semi-Causal                              |                                                                                                                                             | [METALM](https://github.com/microsoft/unilm)                                                     |                                | [2206.06336](https://arxiv.org/abs/2206.06336)                                        | Microsoft         |
| Language Models Can See: Plugging Visual Controls in Text Generation                                                                                                      | GPT2                            |                                          |                                                                                                                                             | [MAGIC](https://github.com/yxuansu/MAGIC)                                                        |                                | [2205.02655](https://arxiv.org/abs/2205.02655)                                        | Tencent           |
| Flamingo: a Visual Language Model for Few-Shot Learning                                                                                                                   | Chinchilla / adapter            |                                          |                                                                                                                                             | [Flamingo](https://github.com/lucidrains/flamingo-pytorch)                                       | Neurips 2022                  | [2204.14198](https://arxiv.org/abs/2204.14198)                                        | DeepMind          |
| Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language                                                                                                   | GPT3, RoBERTa                   |                                          |                                                                                                                                             | [Socratic Models](https://github.com/google-research/google-research/tree/master/socraticmodels) | ICLR 2023                      | [2204.00598](https://arxiv.org/abs/2204.00598)                                        | Google            |
| An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA                                                                                                              | GPT3                            | LLM as KB, ICL                           | evaluate: OKVQA                                                                                                                             | [PICa](https://github.com/microsoft/PICa)                                                        | AAAI 2022                      | [2109.05014](https://arxiv.org/abs/2109.05014)                                        | Microsoft         |
| Multimodal few-shot learning with frozen language models                                                                                                                  | Transforemr-LM-7b (PT on C4)   | ICL                                      | [ConceptualCaptions](https://github.com/google-research-datasets/conceptual-captions)                                                          | [Frozen](https://github.com/ilkerkesen/frozen) (unofficial)                                     | Neurips 2021                   | [2106.13884](https://arxiv.org/abs/2106.13884)                                        | Deepmind          |
| Perceiver: General Perception with Iterative Attention                                                                                                                    | Perceiver                       | latent array                             |                                                                                                                                             |                                                                                               | ICML 2021                      | [2103.03206](https://arxiv.org/abs/2103.03206)                                        | DeepMind          |
| Learning Transferable Visual Models From Natural Language Supervision                                                                                                     | Bert / contrastive learning     |                                          |                                                                                                                                             | [CLIP](https://github.com/openai/CLIP)                                                           | ICML 2021                      | [2103.00020](https://arxiv.org/abs/2103.00020)                                        | OpenAI            |

### Datasets & Benchmarks

| Benchmark                                                                               | Task                                                                                                        | Data                                                                                 | Paper                                                                                         | Preprint                                    | Publication  | Affiliation      |
| --------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------ | ---------------- |
| [MathVista](https://mathvista.github.io/)                                                  | QA (math: IQTest, FuctionQA, PaperQA)                                                                       | self-construct + mixture QA pairs (6K)                                               | MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts          | [2310.02255](https://arxiv.org/abs/2310.02255) |              | Microsoft        |
| [VisIT-Bench](https://visit-bench.github.io/)                                              | QA (general domain)                                                                                         | self-construct (592)                                                                 | VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use | [2308.06595](https://arxiv.org/abs/2308.06595) |              | LAION            |
| [SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard)               | QA (general domain)                                                                                         | self-construct (19K)                                                                 | SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension                        | [2307.16125](https://arxiv.org/abs/2307.16125) |              | Tencent          |
| [MMBench](https://opencompass.org.cn/leaderboard-multimodal)                               | QA (general domain)                                                                                         | mixture (2.9K)                                                                       | MMBench: Is Your Multi-modal Model an All-around Player?                                      | [2307.06281](https://arxiv.org/abs/2307.06281) |              | Shanghai AI Lab. |
| [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) | QA (general domain)                                                                                         | self-construct (2.1K)                                                                | MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models                | [2306.13394](https://arxiv.org/abs/2306.13394) |              | XMU              |
| [M3IT](https://huggingface.co/datasets/MMInstruction/M3IT)                                 | General INST                                                                                                | self-construct INSTs (2.4M)                                                          | M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning               | [2306.04387](https://arxiv.org/abs/2306.04387) |              | HKU              |
| [POPE](https://github.com/RUCAIBox/POPE)                                                   | General (object hallucination)                                                                              |                                                                                      | POPE: Polling-based Object Probing Evaluation for Object Hallucination                        | [2305.10355](https://arxiv.org/abs/2305.10355) |              | RUC              |
| [DataComp](https://github.com/mlfoundations/datacomp)                                      | Curate I-T Pairs                                                                                            | 12.8M I-T pairs                                                                      | DataComp: In search of the next generation of multimodal datasets                             | [2304.14108](https://arxiv.org/abs/2304.14108) |              | DataComp.AI      |
| [MM-Vet](https://github.com/yuweihao/MM-Vet)                                               | General                                                                                                     | [mm-vet.zip](https://github.com/yuweihao/MM-Vet/releases/download/v1/mm-vet.zip)        | MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities                        |                                             |              |                  |
| [INFOSEEK](https://open-vision-language.github.io/infoseek/)                               | VQA                                                                                                         | [OVEN (open domain image)](https://open-vision-language.github.io/oven/) + Human Anno. | Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?       | [2302.11713](https://arxiv.org/abs/2302.11713) |              | Google           |
| [MultiInstruct](https://github.com/VT-NLP/MultiInstruct)                                   | General INST (Grounded Caption, Text Localization, Referring Expression Selection, Question-Image Matching) | self-construct INSTs (62 * (5+5))                                                    | MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning                | [2212.10773](https://arxiv.org/abs/2212.10773) | ACL 2023     | Virginia Tech    |
| [ScienceQA](https://github.com/lupantech/ScienceQA)                                        | QA (elementary and high school science curricula)                                                           | self-construct QA-pairs (21K)                                                        | Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering      | [2209.09513](https://arxiv.org/abs/2209.09513) | NeurIPS 2022 | AI2              |

## Image Generation

### Reading List

*Include some insightful works except for LLM*

| Paper                                                                                                    | Base Language Model               | Code                                                                                            | Publication | Preprint                                    | Affiliation     |
| -------------------------------------------------------------------------------------------------------- | --------------------------------- | ----------------------------------------------------------------------------------------------- | ----------- | ------------------------------------------- | --------------- |
| VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation        | LLaMA, IP-Adapter (*Diffusion*) | [VL-GPT](https://github.com/AILab-CVC/VL-GPT)                                                      |             |                                             | Tencent         |
|     LLMGA: Multimodal Large Language Model based Generation Assistant                                                                                                     |   LLaVA, *Unet*                                |   [LLMGA](https://github.com/dvlab-research/LLMGA)                                                                                              |             |           [2311.16500](https://arxiv.org/abs/2311.16500)                                  |         CUHK        |
| AnyText: Multilingual Visual Text Generation And Editing                                                 | *ControlNet* (OCR)              | [AnyText](https://github.com/tyxsspa/AnyText)                                                      |             | [2311.03054](https://arxiv.org/abs/2311.03054) | Alibaba         |
| MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens                              | *Unet*                          | [MiniGPT-5](https://github.com/eric-ai-lab/MiniGPT-5)                                              |             | [2310.02239](https://arxiv.org/abs/2310.02239) | UCSC            |
| NExT-GPT: Any-to-Any Multimodal LLM                                                                      | Vicuna-7B,*Diffusion*           | [NExT-GPT](https://github.com/NExT-GPT/NExT-GPT)                                                   |             | [2309.05519](https://arxiv.org/abs/2309.05519) | NUS             |
| Generative Pretraining in Multimodality                                                                  | LLaMA，*Diffusion*              | [Emu](https://github.com/baaivision/Emu)                                                           |             | [2307.05222](https://arxiv.org/abs/2307.05222) | BAAI            |
| SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs                            | PaLM2, GPT3.5                     |                                                                                                 |             | [2306.17842](https://arxiv.org/abs/2306.17842) | Google          |
| LayoutGPT: Compositional Visual Planning and Generation with Large Language Models                       | ChatGPT                           | [LayoutGPT](https://github.com/weixi-feng/LayoutGPT)                                               |             | [2305.15393](https://arxiv.org/abs/2305.15393) | UCSB            |
| BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing | Blip2,*u-net*                   | [BLIP-Diffusion](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)            |             | [2305.14720](https://arxiv.org/abs/2305.14720) | Salesforce      |
| CoDi: Any-to-Any Generation via Composable Diffusion                                                     | *Diffusion*                     | [CoDi](https://github.com/microsoft/i-Code/tree/main/i-Code-V3)                                    |             | [2305.11846](https://arxiv.org/abs/2305.11846) | Microsoft & UNC |
| ImageBind: One Embedding Space To Bind Them All                                                          | CLIP                              | [ImageBind](https://github.com/facebookresearch/ImageBind)                                         |             | [2305.05665](https://arxiv.org/abs/2305.05665) | Meta            |
| Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation                 | ChatGPT,*VAE*                   | [Accountable Textual Visual Chat](https://github.com/matrix-alpha/Accountable-Textual-Visual-Chat) |             | [2303.05983](https://arxiv.org/abs/2303.05983) | CUHK            |
| Denoising Diffusion Probabilistic Models                                                                 | *Diffusion*                     | [diffusion](https://github.com/hojonathanho/diffusion)                                             |             | [2006.11239](https://arxiv.org/abs/2006.11239) | UCB             |

## Open-source Projects

- [open-prompts](https://github.com/krea-ai/open-prompts), open-source prompts for text-to-image models.
- [LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessory), LLaMA2-Accessory is an open-source toolkit for pretraining, finetuning and deployment of Large Language Models (LLMs) and multimodal LLMs
- [Gemini-vs-GPT4V](https://github.com/Qi-Zhangyang/Gemini-vs-GPT4V), This paper presents an in-depth qualitative comparative study of two pioneering models: Google's Gemini and OpenAI's GPT-4V(ision).
- [multimodal-maestro](https://github.com/roboflow/multimodal-maestro), Effective prompting for Large Multimodal Models like GPT-4 Vision or LLaVA.
  - *by roboflow, 2023.11*
- [VisCPM](https://github.com/OpenBMB/VisCPM), VisCPM is a family of open-source large multimodal models, which support multimodal conversational capabilities (VisCPM-Chat model) and text-to-image generation capabilities (VisCPM-Paint model) in both Chinese and English
  - *by THU, 2023.07*
  - model: [VisCPM-Chat](https://github.com/OpenBMB/VisCPM#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD), [VisCPM-Paint](https://github.com/OpenBMB/VisCPM#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD)
