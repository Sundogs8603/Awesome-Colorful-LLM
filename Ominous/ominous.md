# Ominous


## Reading List

Papers 

#Multimodal #End2end Understanding+Generation:

| Paper                                                                                                                     | Base Model       | Framework                        | Data                                                                  | Code                                                                                                     | Publication         | Preprint                                                               | Affiliation     |
| ------------------------------------------------------------------------------------------------------------------------- | ------------------------- | -------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------- | ---------------------------------------------------------------------- | --------------- |
|          X-VILA: Cross-Modality Alignment for Large Language Model                          |    vicuna                       |         INST + Diffusion Decoder                         |     mixture (image, video, audio)                                                                 |                                                                                                  |                     | [2405.19335](https://arxiv.org/abs/2405.19335)                            | NVIDIA       |
|          Chameleon: Mixed-Modal Early-Fusion Foundation Models                          |    - (chameleon)                       |         PT + FT (AS + image detokenizer)                         |     mixture (image)                                                                 |                                                                                                  |                     | [2405.09818](https://arxiv.org/abs/2405.09818)                            | Meta       |
| AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling                                                                                                            | LLaMA2                          | INST + NAS-decoder                                       | mixture(image, speech, music)                                                                                                               | [AnyGPT](https://github.com/OpenMOSS/AnyGPT)                                                     |                                                 | [2402.12226](https://arxiv.org/abs/2402.12226)                                        | FDU               |
| LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment                     | -                         | INST                               | mixture (video, infrared, depth, audio)                                                               | [LanguageBind](https://github.com/PKU-YuanGroup/LanguageBind)                                               | ICLR2024            | [2310.01852](https://arxiv.org/abs/2310.01852)                            | PKU             |
| ImageBind: One Embedding Space To Bind Them All                                                          | CLIP                              | Contrastive + Diffusion Decoder | mixture(image, video, audio, depth) | [ImageBind](https://github.com/facebookresearch/ImageBind)                                         |             | [2305.05665](https://arxiv.org/abs/2305.05665) | Meta            |


#Streaming #Real-Time #Online

| Paper                                                                                                                     | Base Model       | Framework                        | Data                                                                  | Code                                                                                                     | Publication         | Preprint                                                               | Affiliation     |
| ------------------------------------------------------------------------------------------------------------------------- | ------------------------- | -------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------- | ---------------------------------------------------------------------- | --------------- |
| VITA: Towards Open-Source Interactive Omni Multimodal LLM                                                                  | Mixtral-8x7B                | special tokens (<1>: audio; <2>: EOS; <3> text)                           |                       mixture                                          |        [VITA](https://github.com/VITA-MLLM/VITA)                                                 |                     | [2408.05211](https://arxiv.org/abs/2408.05211)  | Tencent      |
| VideoLLM-online: Online Large Language Model for Streaming Video                                                                  | Llama2/3                | Multi-turn dialogue + streaming loss                           |                       Ego4D                                          |        [videollm-online](https://github.com/showlab/videollm-online)                                                 |                     | [2406.11816](https://arxiv.org/abs/2406.11816)  |NUS       |
| RT-DETR: DETRs Beat YOLOs on Real-time Object Detection                                                                  | Dino + DETR              | anchor-free                           |                       COCO                                          |        [RT-DETR](https://github.com/lyuwenyu/RT-DETR)                                                 |                     | [2304.08069](https://arxiv.org/abs/2304.08069)  |    Baidu   |
| Streaming Dense Video Captioning                                                                  | GIT/VidSeq + T5              | cluster visual token (memory)                           |                                                                 |        [streaming_dvc](https://github.com/google-research/scenic/tree/main/scenic/projects/streaming_dvc)                                                 |               CVPR2024      | [2304.08069](https://arxiv.org/abs/2404.01297)  |    Google   |
|  Deformable DETR: Deformable Transformers for End-to-End Object Detection                                                                  | ResNet+DETR               | deformable-attention                           |                       COCO                                          |        [Deformable-DETR](https://github.com/fundamentalvision/Deformable-DETR)                                                 |       ICLR2021              | [2010.04159](https://arxiv.org/abs/2010.04159)  |    SenseTime   |

Projects:

- [2024.07] [SAM2](https://github.com/facebookresearch/segment-anything-2), Introducing Meta Segment Anything Model 2 (SAM 2)
    - [2024.08] [segment-anything-2-real-time](https://github.com/Gy920/segment-anything-2-real-time), Run Segment Anything Model 2 on a live video stream
- [2024.06] [LLaVA-Magvit2](https://github.com/lucasjinreal/LLaVA-Magvit2), LLaVA MagVit2: Combines MLLM Understanding and Generation with MagVit2
- [2024.05] [GPT-4o system card](https://openai.com/index/hello-gpt-4o/), Weâ€™re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

## Dataset

- [2024.06] [ShareGPT4Omni Dataset](https://sharegpt4omni.github.io/), ShareGPT4Omni: Towards Building Omni Large Multi-modal Models with Comprehensive Multi-modal Annotations.