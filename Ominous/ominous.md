# Ominous


## Reading List

Papers:

| Paper                                                                                                                     | Base Language Model       | Framework                        | Data                                                                  | Code                                                                                                     | Publication         | Preprint                                                               | Affiliation     |
| ------------------------------------------------------------------------------------------------------------------------- | ------------------------- | -------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------- | ---------------------------------------------------------------------- | --------------- |
|          X-VILA: Cross-Modality Alignment for Large Language Model                          |    vicuna                       |         INST + Diffusion Decoder                         |     mixture (image, video, audio)                                                                 |                                                                                                  |                     | [2405.19335](https://arxiv.org/abs/2405.19335)                            | NVIDIA       |
| AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling                                                                                                            | LLaMA2                          | INST + NAS-decoder                                       | mixture(image, speech, music)                                                                                                               | [AnyGPT](https://github.com/OpenMOSS/AnyGPT)                                                     |                                                 | [2402.12226](https://arxiv.org/abs/2402.12226)                                        | FDU               |
| LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment                     | -                         | INST                               | mixture (video, infrared, depth, audio)                                                               | [LanguageBind](https://github.com/PKU-YuanGroup/LanguageBind)                                               | ICLR2024            | [2310.01852](https://arxiv.org/abs/2310.01852)                            | PKU             |
| ImageBind: One Embedding Space To Bind Them All                                                          | CLIP                              | Contrastive + Diffusion Decoder | mixture(image, video, audio, depth) | [ImageBind](https://github.com/facebookresearch/ImageBind)                                         |             | [2305.05665](https://arxiv.org/abs/2305.05665) | Meta            |

Projects:

- [2024.06] [LLaVA-Magvit2](https://github.com/lucasjinreal/LLaVA-Magvit2), LLaVA MagVit2: Combines MLLM Understanding and Generation with MagVit2
- [2024.05] [GPT-4o system card](https://openai.com/index/hello-gpt-4o/), Weâ€™re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

## Dataset

- [2024.06] [ShareGPT4Omni Dataset](https://sharegpt4omni.github.io/), ShareGPT4Omni: Towards Building Omni Large Multi-modal Models with Comprehensive Multi-modal Annotations.